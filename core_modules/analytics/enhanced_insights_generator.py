"""
å¢å¼ºæ´å¯Ÿç”Ÿæˆå™¨
å°†ç»Ÿè®¡åˆ†æç»“æœä¸LLMç”Ÿæˆçš„æ´å¯Ÿè¿›è¡Œæ·±åº¦æ•´åˆ
"""

import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import pandas as pd

logger = logging.getLogger(__name__)

class EnhancedInsightsGenerator:
    """å¢å¼ºæ´å¯Ÿç”Ÿæˆå™¨ - æ•´åˆç»Ÿè®¡åˆ†æä¸ä¸šåŠ¡æ´å¯Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºæ´å¯Ÿç”Ÿæˆå™¨"""
        self.business_thresholds = {
            'ä¸ªäººè´·æ¬¾': {
                'excellent': 10e8,  # 10äº¿ä»¥ä¸Šä¸ºä¼˜ç§€
                'good': 5e8,        # 5äº¿ä»¥ä¸Šä¸ºè‰¯å¥½
                'poor': 1e8         # 1äº¿ä»¥ä¸‹éœ€å…³æ³¨
            },
            'å¯¹å…¬è´·æ¬¾': {
                'excellent': 50e8,
                'good': 20e8,
                'poor': 5e8
            }
        }
        
        self.statistical_significance = {
            'high_variance_cv': 1.0,      # å˜å¼‚ç³»æ•°>1.0ä¸ºé«˜æ–¹å·®
            'outlier_threshold': 2.0,     # è¶…è¿‡2å€æ ‡å‡†å·®ä¸ºå¼‚å¸¸å€¼
            'concentration_threshold': 0.5 # å‰å‡ åå æ¯”>50%ä¸ºé«˜é›†ä¸­åº¦
        }

    def _is_id_field(self, column_name: str) -> bool:
        """
        åˆ¤æ–­å­—æ®µæ˜¯å¦ä¸ºIDç±»å­—æ®µï¼Œä¸é€‚åˆè¿›è¡Œä¸šåŠ¡åˆ†æ

        Args:
            column_name: å­—æ®µåç§°

        Returns:
            bool: Trueè¡¨ç¤ºæ˜¯IDç±»å­—æ®µï¼Œåº”è¯¥è·³è¿‡åˆ†æ
        """
        # IDå­—æ®µè¯†åˆ«æ¨¡å¼
        id_field_patterns = [
            'id', 'ID', 'ç¼–å·', 'ä»£ç ', '_no', '_code', '_id',
            'statistics_dt', 'æ•°æ®æ—¥æœŸ', 'ç»Ÿè®¡æ—¥æœŸ',
            'CF_A', 'CF_B', 'CF_C', 'CF_D', 'CF_E', 'CF_F', 'CF_G', 'CF_H', 'CF_I'
        ]

        # æ£€æŸ¥å­—æ®µåæ˜¯å¦åŒ…å«IDç±»æ¨¡å¼
        column_lower = column_name.lower()
        for pattern in id_field_patterns:
            if pattern.lower() in column_lower or column_name == pattern:
                return True

        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯ç¼–ç å­—æ®µï¼ˆå¦‚CF_å¼€å¤´çš„ç¼–ç ï¼‰
        if column_name.startswith('CF_') and len(column_name) <= 4:
            return True

        # æ£€æŸ¥æ˜¯å¦ä»¥å¸¸è§IDåç¼€ç»“å°¾
        id_suffixes = ['_id', '_no', '_code', '_num', '_number']
        for suffix in id_suffixes:
            if column_name.lower().endswith(suffix):
                return True

        return False

    def _get_smart_institution_identifier(self, record: Dict[str, Any]) -> str:
        """
        æ™ºèƒ½è·å–æœºæ„æ ‡è¯†ï¼šä¼˜å…ˆæœºæ„åç§°ï¼Œå…¶æ¬¡å®¢æˆ·IDï¼Œæœ€åä½¿ç”¨è®°å½•æ ‡è¯†

        Args:
            record: æ•°æ®è®°å½•

        Returns:
            str: æœºæ„æ ‡è¯†ï¼ˆæœºæ„åç§°ã€å®¢æˆ·IDæˆ–è®°å½•æ ‡è¯†ï¼‰
        """
        # 1. å°è¯•è·å–æœºæ„åç§°ï¼ˆå¤šç§å¯èƒ½çš„å­—æ®µåï¼‰
        institution_fields = [
            'host_org_name',      # å®é™…å­—æ®µå
            'è´¦åŠ¡æœºæ„åç§°',        # ä¸­æ–‡å­—æ®µå
            'org_name',           # ç®€åŒ–å­—æ®µå
            'branch_name',        # åˆ†æ”¯æœºæ„å
            'æœºæ„åç§°',           # å…¶ä»–å¯èƒ½çš„ä¸­æ–‡å
            'institution_name'    # è‹±æ–‡æ ‡å‡†å
        ]

        for field in institution_fields:
            if field in record and record[field]:
                value = str(record[field]).strip()
                # è¿‡æ»¤æ— æ•ˆå€¼
                if value.lower() not in ['nan', 'null', 'none', '', 'æœªçŸ¥', 'unknown']:
                    return f"æœºæ„:{value}"

        # 2. å¦‚æœæ²¡æœ‰æœºæ„åç§°ï¼Œå°è¯•ä½¿ç”¨å®¢æˆ·IDä½œä¸ºæ ‡è¯†
        customer_id_fields = [
            'CUST_ID',           # å®¢æˆ·ID
            'å®¢æˆ·ç¼–å·',          # ä¸­æ–‡å®¢æˆ·ç¼–å·
            'customer_id',       # è‹±æ–‡å®¢æˆ·ID
            'cust_no'           # ç®€åŒ–å®¢æˆ·å·
        ]

        for field in customer_id_fields:
            if field in record and record[field]:
                value = str(record[field]).strip()
                if value.lower() not in ['nan', 'null', 'none', '']:
                    return f"å®¢æˆ·ID:{value}"

        # 3. æœ€åå°è¯•ä½¿ç”¨å…¶ä»–æ ‡è¯†å­—æ®µ
        identifier_fields = [
            'hold_org_id',       # æœºæ„ID
            'statistics_dt',     # ç»Ÿè®¡æ—¥æœŸ
            'open_dt'           # å¼€æˆ·æ—¥æœŸ
        ]

        for field in identifier_fields:
            if field in record and record[field]:
                value = str(record[field]).strip()
                if value.lower() not in ['nan', 'null', 'none', '']:
                    return f"{field}:{value}"

        # 4. å¦‚æœæ‰€æœ‰å­—æ®µéƒ½æ— æ•ˆï¼Œè¿”å›è®°å½•æ‘˜è¦
        available_fields = [k for k, v in record.items() if v is not None and str(v).strip()]
        if available_fields:
            return f"è®°å½•(å«{len(available_fields)}ä¸ªå­—æ®µ)"
        else:
            return "æ•°æ®è®°å½•"

    def generate_enhanced_insights(self,
                                 agent_response: str,
                                 original_insights: Optional[str],
                                 original_recommendations: Optional[str],
                                 statistics: Dict[str, Any],
                                 data_tables: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¢å¼ºæ´å¯Ÿï¼Œæ•´åˆåŸå§‹æ´å¯Ÿå’Œç»Ÿè®¡åˆ†æ

        Args:
            agent_response: Agentçš„åŸå§‹å“åº”
            original_insights: åŸå§‹æ´å¯Ÿæ–‡æœ¬
            original_recommendations: åŸå§‹å»ºè®®æ–‡æœ¬
            statistics: ç»Ÿè®¡åˆ†æç»“æœ
            data_tables: åŸå§‹æ•°æ®è¡¨

        Returns:
            Dict: å¢å¼ºæ´å¯Ÿç»“æœ
        """
        try:
            logger.info("å¼€å§‹ç”Ÿæˆå¢å¼ºæ´å¯Ÿ")
            logger.info(f"è¾“å…¥å‚æ•° - statisticsæœ‰æ•ˆ: {bool(statistics)}, data_tablesæœ‰æ•ˆ: {bool(data_tables)}")

            # 1. åˆ†æç»Ÿè®¡æ•°æ®ï¼Œç”Ÿæˆæ•°æ®é©±åŠ¨çš„æ´å¯Ÿ
            statistical_insights = self._generate_statistical_insights(statistics, data_tables)
            logger.info(f"ç»Ÿè®¡æ´å¯Ÿç”Ÿæˆå®Œæˆ: {len(statistical_insights)}ä¸ª")

            # 2. è§£æåŸå§‹æ´å¯Ÿï¼Œæå–å…³é”®ä¿¡æ¯
            parsed_insights = self._parse_original_insights(original_insights, original_recommendations)
            logger.info(f"åŸå§‹æ´å¯Ÿè§£æå®Œæˆ: {len(parsed_insights)}ä¸ª")

            # 3. å°†ç»Ÿè®¡æ•°æ®ä¸æ´å¯Ÿè¿›è¡Œå…³è”
            enhanced_insights = self._merge_insights_with_statistics(
                parsed_insights, statistical_insights, statistics, data_tables
            )
            logger.info(f"æ´å¯Ÿåˆå¹¶å®Œæˆ: {len(enhanced_insights)}ä¸ª")

            # 4. ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
            executive_summary = self._generate_executive_summary(enhanced_insights, statistics)
            logger.info(f"æ‰§è¡Œæ‘˜è¦ç”Ÿæˆå®Œæˆ: {bool(executive_summary)}")

            # 5. ç”Ÿæˆå¯æ“ä½œçš„å»ºè®®
            actionable_recommendations = self._generate_actionable_recommendations(
                enhanced_insights, statistics, data_tables
            )
            logger.info(f"å¯æ“ä½œå»ºè®®ç”Ÿæˆå®Œæˆ: {len(actionable_recommendations)}ä¸ª")

            # 6. åˆ›å»ºç»Ÿè®¡æ‘˜è¦
            statistical_summary = self._create_statistical_summary(statistics)

            # ğŸ”§ é‡æ–°è®¾è®¡æ›´ç®€æ´çš„æ•°æ®ç»“æ„
            result = {
                'summary': self._create_business_summary(enhanced_insights, statistics, data_tables),
                'key_insights': self._create_key_insights(enhanced_insights),
                'recommendations': self._create_simple_recommendations(actionable_recommendations),
                'data_overview': self._create_data_overview(statistics, data_tables),
                'metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'total_insights': len(enhanced_insights),
                    'total_recommendations': len(actionable_recommendations),
                    'analysis_timestamp': datetime.now().isoformat()
                }
            }

            logger.info(f"å¢å¼ºæ´å¯Ÿç”Ÿæˆå®Œæˆ: {len(enhanced_insights)}ä¸ªæ´å¯Ÿ, {len(actionable_recommendations)}ä¸ªå»ºè®®")
            return result

        except Exception as e:
            logger.error(f"ç”Ÿæˆå¢å¼ºæ´å¯Ÿå¤±è´¥: {e}", exc_info=True)
            return self._create_fallback_insights(original_insights, original_recommendations)
    
    def _generate_statistical_insights(self, statistics: Dict[str, Any],
                                     data_tables: Dict[str, List[Dict]]) -> List[Dict[str, Any]]:
        """åŸºäºç»Ÿè®¡æ•°æ®ç”Ÿæˆæ´å¯Ÿ"""
        insights = []

        logger.info(f"å¼€å§‹ç”Ÿæˆç»Ÿè®¡æ´å¯Ÿ - statistics: {bool(statistics)}")

        if not statistics or 'statistics' not in statistics:
            logger.warning("ç»Ÿè®¡æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
            return insights

        logger.info(f"ç»Ÿè®¡æ•°æ®åŒ…å« {len(statistics['statistics'])} ä¸ªè¡¨")

        for table_name, table_stats in statistics['statistics'].items():
            logger.info(f"å¤„ç†è¡¨: {table_name}")

            if 'column_statistics' not in table_stats:
                logger.warning(f"è¡¨ {table_name} ç¼ºå°‘ column_statistics")
                continue

            table_data = data_tables.get(table_name, [])
            logger.info(f"è¡¨ {table_name} æœ‰ {len(table_data)} æ¡æ•°æ®è®°å½•")

            # åˆ†ææ¯ä¸ªæ•°å€¼åˆ—
            for column_name, column_stats in table_stats['column_statistics'].items():
                logger.info(f"åˆ†æåˆ—: {column_name}")

                if 'error' in column_stats:
                    logger.warning(f"åˆ— {column_name} æœ‰é”™è¯¯: {column_stats['error']}")
                    continue

                # 1. è¯†åˆ«æœ€ä½³è¡¨ç°è€…
                if 'max_record' in column_stats and column_stats['max_record']:
                    max_insight = self._create_performance_leader_insight(
                        column_name, column_stats, table_name
                    )
                    if max_insight:
                        insights.append(max_insight)
                        logger.info(f"ç”Ÿæˆä¸šç»©é¢†å…ˆè€…æ´å¯Ÿ: {max_insight['title']}")

                # 2. è¯†åˆ«æ•°æ®åˆ†å¸ƒç‰¹å¾
                variance_insight = self._analyze_data_distribution(
                    column_name, column_stats, table_name
                )
                if variance_insight:
                    insights.append(variance_insight)
                    logger.info(f"ç”Ÿæˆåˆ†å¸ƒç‰¹å¾æ´å¯Ÿ: {variance_insight['title']}")

                # 3. è¯†åˆ«ä¸šåŠ¡é£é™©
                risk_insight = self._identify_business_risks(
                    column_name, column_stats, table_data, table_name
                )
                if risk_insight:
                    insights.append(risk_insight)
                    logger.info(f"ç”Ÿæˆä¸šåŠ¡é£é™©æ´å¯Ÿ: {risk_insight['title']}")

        logger.info(f"ç»Ÿè®¡æ´å¯Ÿç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(insights)} ä¸ªæ´å¯Ÿ")
        return insights
    
    def _create_performance_leader_insight(self, column_name: str,
                                         column_stats: Dict[str, Any],
                                         table_name: str) -> Optional[Dict[str, Any]]:
        """åˆ›å»ºä¸šç»©é¢†å…ˆè€…æ´å¯Ÿ"""
        try:
            # ğŸš¨ IDå­—æ®µè¿‡æ»¤ï¼šæ’é™¤ä¸é€‚åˆåˆ†æçš„å­—æ®µç±»å‹
            if self._is_id_field(column_name):
                logger.info(f"è·³è¿‡IDç±»å­—æ®µåˆ†æ: {column_name}")
                return None

            max_record = column_stats['max_record']
            max_value = column_stats['max']
            mean_value = column_stats['mean']

            # ğŸ”§ æ™ºèƒ½è·å–æœºæ„æ ‡è¯†ï¼šä¼˜å…ˆæœºæ„åç§°ï¼Œå…¶æ¬¡å®¢æˆ·IDï¼Œæœ€åä½¿ç”¨è®°å½•æ ‡è¯†
            institution_name = self._get_smart_institution_identifier(max_record)

            # è®¡ç®—ç›¸å¯¹è¡¨ç°
            vs_average_multiple = max_value / mean_value if mean_value > 0 else 0
            
            return {
                'id': f"leader_{column_name}_{hash(institution_name) % 10000}",
                'type': 'performance_leader',
                'priority': 'high',
                'title': f"{institution_name}åœ¨{column_name}æ–¹é¢è¡¨ç°æœ€ä½³",
                'description': f"{institution_name}ä»¥{self._format_currency(max_value)}çš„{column_name}ä½å±…ç¬¬ä¸€",
                'supporting_statistics': {
                    'primary_metric': {
                        'value': max_value,
                        'label': column_name,
                        'format': 'currency',
                        'institution': institution_name
                    },
                    'comparative_metrics': [
                        {
                            'label': 'vs å¹³å‡å€¼',
                            'value': vs_average_multiple,
                            'format': 'multiplier'
                        }
                    ]
                },
                'evidence': {
                    'data_source': table_name,
                    'record': max_record,
                    'calculation': f"MAX({column_name})"
                },
                'actions': [
                    {
                        'type': 'view_details',
                        'label': 'æŸ¥çœ‹è¯¦æƒ…',
                        'target': f"table_row:{institution_name}"
                    },
                    {
                        'type': 'analyze_factors',
                        'label': 'æˆåŠŸå› ç´ åˆ†æ',
                        'target': f"institution:{institution_name}"
                    }
                ]
            }
        except Exception as e:
            logger.error(f"åˆ›å»ºä¸šç»©é¢†å…ˆè€…æ´å¯Ÿå¤±è´¥: {e}")
            return None
    
    def _analyze_data_distribution(self, column_name: str,
                                 column_stats: Dict[str, Any],
                                 table_name: str) -> Optional[Dict[str, Any]]:
        """åˆ†ææ•°æ®åˆ†å¸ƒç‰¹å¾"""
        try:
            # ğŸš¨ IDå­—æ®µè¿‡æ»¤ï¼šæ’é™¤ä¸é€‚åˆåˆ†æçš„å­—æ®µç±»å‹
            if self._is_id_field(column_name):
                logger.info(f"è·³è¿‡IDç±»å­—æ®µåˆ†å¸ƒåˆ†æ: {column_name}")
                return None

            mean_value = column_stats['mean']
            std_value = column_stats['std']

            if mean_value <= 0:
                return None
            
            # è®¡ç®—å˜å¼‚ç³»æ•°
            cv = std_value / mean_value
            
            if cv > self.statistical_significance['high_variance_cv']:
                return {
                    'id': f"variance_{column_name}_{hash(table_name) % 10000}",
                    'type': 'high_variance',
                    'priority': 'medium',
                    'title': f"{column_name}åˆ†å¸ƒä¸å‡ï¼Œå­˜åœ¨è¾ƒå¤§å·®å¼‚",
                    'description': f"{column_name}çš„å˜å¼‚ç³»æ•°ä¸º{cv:.2f}ï¼Œè¡¨æ˜å„æœºæ„é—´å·®å¼‚è¾ƒå¤§",
                    'supporting_statistics': {
                        'primary_metric': {
                            'value': cv,
                            'label': 'å˜å¼‚ç³»æ•°',
                            'format': 'decimal'
                        },
                        'comparative_metrics': [
                            {
                                'label': 'æ ‡å‡†å·®',
                                'value': std_value,
                                'format': 'currency'
                            },
                            {
                                'label': 'å‡å€¼',
                                'value': mean_value,
                                'format': 'currency'
                            }
                        ]
                    },
                    'evidence': {
                        'data_source': table_name,
                        'calculation': f"STD({column_name}) / MEAN({column_name})"
                    },
                    'actions': [
                        {
                            'type': 'view_distribution',
                            'label': 'æŸ¥çœ‹åˆ†å¸ƒå›¾',
                            'target': f"distribution:{column_name}"
                        },
                        {
                            'type': 'analyze_outliers',
                            'label': 'å¼‚å¸¸å€¼åˆ†æ',
                            'target': f"outliers:{column_name}"
                        }
                    ]
                }
        except Exception as e:
            logger.error(f"åˆ†ææ•°æ®åˆ†å¸ƒå¤±è´¥: {e}")
            return None
    
    def _identify_business_risks(self, column_name: str,
                               column_stats: Dict[str, Any],
                               table_data: List[Dict],
                               table_name: str) -> Optional[Dict[str, Any]]:
        """è¯†åˆ«ä¸šåŠ¡é£é™©"""
        try:
            # ğŸš¨ IDå­—æ®µè¿‡æ»¤ï¼šæ’é™¤ä¸é€‚åˆåˆ†æçš„å­—æ®µç±»å‹
            if self._is_id_field(column_name):
                logger.info(f"è·³è¿‡IDç±»å­—æ®µé£é™©åˆ†æ: {column_name}")
                return None

            min_value = column_stats['min']
            mean_value = column_stats['mean']

            # è¯†åˆ«ä¸šåŠ¡ç±»å‹
            business_type = self._identify_business_type(table_name, column_name)
            thresholds = self.business_thresholds.get(business_type, {})
            
            if not thresholds:
                return None
            
            # ç»Ÿè®¡ä½äºé˜ˆå€¼çš„æœºæ„æ•°é‡
            poor_threshold = thresholds.get('poor', 0)
            poor_performers = [
                record for record in table_data 
                if record.get(column_name, 0) < poor_threshold
            ]
            
            if len(poor_performers) > 0:
                return {
                    'id': f"risk_{column_name}_{hash(table_name) % 10000}",
                    'type': 'business_risk',
                    'priority': 'high',
                    'title': f"{len(poor_performers)}ä¸ªæœºæ„{column_name}ä½äºé¢„è­¦çº¿",
                    'description': f"æœ‰{len(poor_performers)}ä¸ªæœºæ„çš„{column_name}ä½äº{self._format_currency(poor_threshold)}ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨",
                    'supporting_statistics': {
                        'primary_metric': {
                            'value': len(poor_performers),
                            'label': 'é£é™©æœºæ„æ•°é‡',
                            'format': 'integer'
                        },
                        'comparative_metrics': [
                            {
                                'label': 'é¢„è­¦é˜ˆå€¼',
                                'value': poor_threshold,
                                'format': 'currency'
                            },
                            {
                                'label': 'æœ€ä½å€¼',
                                'value': min_value,
                                'format': 'currency'
                            }
                        ]
                    },
                    'evidence': {
                        'data_source': table_name,
                        'affected_records': [r.get('è´¦åŠ¡æœºæ„åç§°', 'æœªçŸ¥') for r in poor_performers[:5]],
                        'calculation': f"COUNT({column_name} < {poor_threshold})"
                    },
                    'actions': [
                        {
                            'type': 'view_list',
                            'label': 'æŸ¥çœ‹é£é™©åå•',
                            'target': f"filter:{column_name}<{poor_threshold}"
                        },
                        {
                            'type': 'risk_analysis',
                            'label': 'é£é™©åˆ†ææŠ¥å‘Š',
                            'target': f"risk_report:{column_name}"
                        }
                    ]
                }
        except Exception as e:
            logger.error(f"è¯†åˆ«ä¸šåŠ¡é£é™©å¤±è´¥: {e}")
            return None
    
    def _parse_original_insights(self, insights: Optional[str], 
                               recommendations: Optional[str]) -> List[Dict[str, Any]]:
        """è§£æåŸå§‹æ´å¯Ÿæ–‡æœ¬"""
        parsed = []
        
        if insights:
            # ç®€å•çš„æ–‡æœ¬è§£æï¼Œå®é™…å¯ä»¥ç”¨æ›´å¤æ‚çš„NLP
            insight_lines = insights.split('\n')
            for i, line in enumerate(insight_lines):
                if line.strip():
                    parsed.append({
                        'id': f"original_insight_{i}",
                        'type': 'llm_generated',
                        'priority': 'medium',
                        'title': line.strip(),
                        'description': line.strip(),
                        'source': 'llm'
                    })
        
        return parsed
    
    def _merge_insights_with_statistics(self, parsed_insights: List[Dict], 
                                      statistical_insights: List[Dict],
                                      statistics: Dict[str, Any],
                                      data_tables: Dict[str, List[Dict]]) -> List[Dict[str, Any]]:
        """åˆå¹¶æ´å¯Ÿä¸ç»Ÿè®¡æ•°æ®"""
        # ä¼˜å…ˆä½¿ç”¨ç»Ÿè®¡é©±åŠ¨çš„æ´å¯Ÿï¼Œè¡¥å……LLMç”Ÿæˆçš„æ´å¯Ÿ
        merged = statistical_insights.copy()
        
        # æ·»åŠ æ²¡æœ‰ç»Ÿè®¡æ”¯æ’‘çš„LLMæ´å¯Ÿ
        for insight in parsed_insights:
            if not self._has_similar_insight(insight, statistical_insights):
                merged.append(insight)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        return sorted(merged, key=lambda x: self._get_priority_score(x['priority']), reverse=True)
    
    def _has_similar_insight(self, insight: Dict, existing_insights: List[Dict]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼çš„æ´å¯Ÿ"""
        insight_keywords = set(insight['title'].lower().split())
        
        for existing in existing_insights:
            existing_keywords = set(existing['title'].lower().split())
            # å¦‚æœæœ‰50%ä»¥ä¸Šçš„å…³é”®è¯é‡å ï¼Œè®¤ä¸ºæ˜¯ç›¸ä¼¼æ´å¯Ÿ
            overlap = len(insight_keywords & existing_keywords)
            if overlap / len(insight_keywords) > 0.5:
                return True
        return False
    
    def _get_priority_score(self, priority: str) -> int:
        """è·å–ä¼˜å…ˆçº§åˆ†æ•°"""
        scores = {'high': 3, 'medium': 2, 'low': 1}
        return scores.get(priority, 1)
    
    def _generate_executive_summary(self, insights: List[Dict],
                                  statistics: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        logger.info(f"ç”Ÿæˆæ‰§è¡Œæ‘˜è¦ - æ´å¯Ÿæ•°é‡: {len(insights)}")

        if not insights:
            logger.warning("æ²¡æœ‰æ´å¯Ÿå¯ç”¨äºç”Ÿæˆæ‰§è¡Œæ‘˜è¦")
            return {
                'key_finding': 'æ•°æ®åˆ†æå®Œæˆï¼Œæœªå‘ç°ç‰¹æ®Šæ´å¯Ÿ',
                'supporting_data': {},
                'evidence_links': {},
                'impact_level': 'medium'
            }

        # æ‰¾åˆ°æœ€é‡è¦çš„æ´å¯Ÿ
        top_insight = insights[0] if insights else None

        if not top_insight:
            logger.warning("é¡¶çº§æ´å¯Ÿä¸ºç©º")
            return {
                'key_finding': 'æ•°æ®åˆ†æå®Œæˆ',
                'supporting_data': {},
                'evidence_links': {},
                'impact_level': 'medium'
            }

        summary = {
            'key_finding': top_insight.get('title', 'æ•°æ®åˆ†æå®Œæˆ'),
            'supporting_data': top_insight.get('supporting_statistics', {}),
            'evidence_links': top_insight.get('evidence', {}),
            'impact_level': top_insight.get('priority', 'medium')
        }

        logger.info(f"æ‰§è¡Œæ‘˜è¦ç”Ÿæˆå®Œæˆ: {summary['key_finding']}")
        return summary
    
    def _generate_actionable_recommendations(self, insights: List[Dict],
                                           statistics: Dict[str, Any],
                                           data_tables: Dict[str, List[Dict]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå¯æ“ä½œçš„å»ºè®®"""
        recommendations = []

        logger.info(f"ç”Ÿæˆå¯æ“ä½œå»ºè®® - æ´å¯Ÿæ•°é‡: {len(insights)}")

        # åŸºäºé£é™©æ´å¯Ÿç”Ÿæˆå»ºè®®
        risk_insights = [i for i in insights if i.get('type') == 'business_risk']
        logger.info(f"é£é™©æ´å¯Ÿæ•°é‡: {len(risk_insights)}")

        for risk in risk_insights:
            try:
                risk_count = risk.get('supporting_statistics', {}).get('primary_metric', {}).get('value', 0)
                recommendations.append({
                    'id': f"rec_{risk['id']}",
                    'priority': 'high',
                    'title': 'åˆ¶å®šé£é™©æœºæ„æå‡è®¡åˆ’',
                    'description': f"é’ˆå¯¹{risk_count}ä¸ªé£é™©æœºæ„ï¼Œå»ºè®®åˆ¶å®šä¸“é¡¹æå‡è®¡åˆ’",
                    'supporting_statistics': risk.get('supporting_statistics', {}),
                    'actions': [
                        {
                            'type': 'create_plan',
                            'label': 'åˆ¶å®šæå‡è®¡åˆ’'
                        },
                        {
                            'type': 'assign_resources',
                            'label': 'åˆ†é…èµ„æºæ”¯æŒ'
                        }
                    ]
                })
                logger.info(f"ç”Ÿæˆé£é™©å»ºè®®: {recommendations[-1]['title']}")
            except Exception as e:
                logger.error(f"ç”Ÿæˆé£é™©å»ºè®®å¤±è´¥: {e}")

        # åŸºäºä¼˜ç§€è¡¨ç°ç”Ÿæˆå»ºè®®
        leader_insights = [i for i in insights if i.get('type') == 'performance_leader']
        logger.info(f"é¢†å…ˆè€…æ´å¯Ÿæ•°é‡: {len(leader_insights)}")

        for leader in leader_insights:
            try:
                institution = leader.get('supporting_statistics', {}).get('primary_metric', {}).get('institution', 'ä¼˜ç§€æœºæ„')
                recommendations.append({
                    'id': f"rec_{leader['id']}",
                    'priority': 'medium',
                    'title': 'æ¨å¹¿ä¼˜ç§€ç»éªŒ',
                    'description': f"å°†{institution}çš„æˆåŠŸç»éªŒæ¨å¹¿åˆ°å…¶ä»–æœºæ„",
                    'supporting_statistics': leader.get('supporting_statistics', {}),
                    'actions': [
                        {
                            'type': 'best_practice',
                            'label': 'æ€»ç»“æœ€ä½³å®è·µ'
                        },
                        {
                            'type': 'knowledge_sharing',
                            'label': 'ç»„ç»‡ç»éªŒåˆ†äº«'
                        }
                    ]
                })
                logger.info(f"ç”Ÿæˆæ¨å¹¿å»ºè®®: {recommendations[-1]['title']}")
            except Exception as e:
                logger.error(f"ç”Ÿæˆæ¨å¹¿å»ºè®®å¤±è´¥: {e}")

        # åŸºäºåˆ†å¸ƒç‰¹å¾ç”Ÿæˆå»ºè®®
        variance_insights = [i for i in insights if i.get('type') == 'high_variance']
        logger.info(f"åˆ†å¸ƒç‰¹å¾æ´å¯Ÿæ•°é‡: {len(variance_insights)}")

        for variance in variance_insights:
            try:
                recommendations.append({
                    'id': f"rec_{variance['id']}",
                    'priority': 'medium',
                    'title': 'ä¼˜åŒ–ä¸šåŠ¡åˆ†å¸ƒ',
                    'description': 'é’ˆå¯¹ä¸šåŠ¡åˆ†å¸ƒä¸å‡çš„æƒ…å†µï¼Œå»ºè®®åˆ¶å®šå‡è¡¡å‘å±•ç­–ç•¥',
                    'supporting_statistics': variance.get('supporting_statistics', {}),
                    'actions': [
                        {
                            'type': 'balance_strategy',
                            'label': 'åˆ¶å®šå‡è¡¡ç­–ç•¥'
                        },
                        {
                            'type': 'resource_allocation',
                            'label': 'ä¼˜åŒ–èµ„æºé…ç½®'
                        }
                    ]
                })
                logger.info(f"ç”Ÿæˆåˆ†å¸ƒä¼˜åŒ–å»ºè®®: {recommendations[-1]['title']}")
            except Exception as e:
                logger.error(f"ç”Ÿæˆåˆ†å¸ƒä¼˜åŒ–å»ºè®®å¤±è´¥: {e}")

        logger.info(f"å¯æ“ä½œå»ºè®®ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(recommendations)} ä¸ªå»ºè®®")
        return recommendations
    
    def _create_statistical_summary(self, statistics: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºç»Ÿè®¡æ‘˜è¦"""
        if not statistics or 'summary' not in statistics:
            return {}
        
        return {
            'tables_analyzed': statistics['summary'].get('analyzed_tables', 0),
            'insights_generated': len(statistics.get('statistics', {})),
            'confidence_score': 0.9,
            'analysis_timestamp': statistics['summary'].get('analyzed_at', '')
        }
    
    def _create_fallback_insights(self, original_insights: Optional[str],
                                original_recommendations: Optional[str]) -> Dict[str, Any]:
        """åˆ›å»ºå¤‡ç”¨æ´å¯Ÿï¼ˆå½“å¢å¼ºç”Ÿæˆå¤±è´¥æ—¶ï¼‰"""
        logger.warning("ä½¿ç”¨å¤‡ç”¨æ´å¯Ÿæ¨¡å¼")

        return {
            'executive_summary': {
                'key_finding': original_insights or 'æ•°æ®åˆ†æå®Œæˆ',
                'supporting_data': {},
                'evidence_links': {},
                'impact_level': 'medium'
            },
            'insights': [],
            'recommendations': [],
            'statistical_summary': {},
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'insights_count': 0,
                'recommendations_count': 0,
                'analysis_timestamp': datetime.now().isoformat(),
                'fallback_mode': True
            }
        }
    
    def _identify_business_type(self, table_name: str, column_name: str) -> str:
        """è¯†åˆ«ä¸šåŠ¡ç±»å‹"""
        if 'ä¸ªäººè´·æ¬¾' in table_name or 'ä¸ªäºº' in column_name:
            return 'ä¸ªäººè´·æ¬¾'
        elif 'å¯¹å…¬' in table_name or 'ä¼ä¸š' in column_name:
            return 'å¯¹å…¬è´·æ¬¾'
        else:
            return 'ä¸ªäººè´·æ¬¾'  # é»˜è®¤
    
    def _format_currency(self, value: float) -> str:
        """æ ¼å¼åŒ–è´§å¸"""
        if value >= 1e8:
            return f"{value/1e8:.1f}äº¿å…ƒ"
        elif value >= 1e4:
            return f"{value/1e4:.1f}ä¸‡å…ƒ"
        else:
            return f"{value:.0f}å…ƒ"

# å…¨å±€å®ä¾‹
enhanced_insights_generator = EnhancedInsightsGenerator()
