#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DataProxy çº¯LLMé©±åŠ¨çš„æ™ºèƒ½æ•°æ®å¯¼å…¥ç³»ç»Ÿ

å®Œå…¨åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½åŒ–æ•°æ®åˆ†æå’Œå¯¼å…¥å¼•æ“
- é›¶è§„åˆ™ç¼–ç ï¼šæ‰€æœ‰é€»è¾‘éƒ½é€šè¿‡LLMæ¨ç†å®ç°
- å®Œå…¨æ™ºèƒ½åŒ–ï¼šä»æ–‡ä»¶è¯†åˆ«åˆ°æ•°æ®åº“è®¾è®¡å…¨éƒ¨ç”±LLMå®Œæˆ
- æ— é™åˆ¶åˆ†æï¼šç§»é™¤è¶…æ—¶é™åˆ¶ï¼Œå…è®¸LLMæ·±åº¦åˆ†æ
- è‡ªä¸»å­¦ä¹ ï¼šLLMè‡ªä¸»å‘ç°æ•°æ®æ¨¡å¼å’Œä¸šåŠ¡è§„åˆ™
"""

import os
import sqlite3
import pandas as pd
import json
import re
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

# LLM API ç›¸å…³å¯¼å…¥ - ä½¿ç”¨ç›´æ¥requestsé¿å…LangChainé—®é¢˜
import requests
import time

# å®Œå…¨ç¦ç”¨æ—¥å¿—è®°å½•ï¼Œé¿å…I/Oé”™è¯¯
class SafeLogger:
    """å®‰å…¨çš„æ—¥å¿—è®°å½•å™¨ï¼Œé¿å…I/Oé”™è¯¯"""
    def info(self, msg, *args, **kwargs):
        # ç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°ï¼Œé¿å…æ—¥å¿—ç³»ç»Ÿé—®é¢˜
        try:
            print(f"â„¹ï¸  {msg}")
        except:
            pass

    def warning(self, msg, *args, **kwargs):
        try:
            print(f"âš ï¸  {msg}")
        except:
            pass

    def error(self, msg, *args, **kwargs):
        try:
            print(f"âŒ {msg}")
        except:
            pass

    def debug(self, msg, *args, **kwargs):
        # è·³è¿‡debugä¿¡æ¯
        pass

logger = SafeLogger()


class IntelligentDataImporter:
    """
    çº¯LLMé©±åŠ¨çš„æ™ºèƒ½æ•°æ®å¯¼å…¥ç³»ç»Ÿ

    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. å®Œå…¨LLMé©±åŠ¨ - æ‰€æœ‰åˆ†æé€»è¾‘éƒ½é€šè¿‡LLMå®ç°
    2. é›¶è§„åˆ™ç¼–ç  - ä¸ä½¿ç”¨ä»»ä½•ç¡¬ç¼–ç è§„åˆ™æˆ–æ¨¡å¼åŒ¹é…
    3. æ— è¶…æ—¶é™åˆ¶ - å…è®¸LLMå……åˆ†åˆ†æå¤æ‚æ•°æ®
    4. è‡ªä¸»å­¦ä¹  - LLMè‡ªä¸»å‘ç°æ•°æ®æ¨¡å¼å’Œä¸šåŠ¡è§„åˆ™
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–çº¯LLMæ™ºèƒ½æ•°æ®å¯¼å…¥ç³»ç»Ÿ

        Args:
            api_key: LLM APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è·å–
        """
        # LLMé…ç½®å‚æ•° (è¿›ä¸€æ­¥å‡å°‘ä»¥é¿å…ChunkedEncodingError)
        self.max_tokens_per_request = 4000   # è¿›ä¸€æ­¥å‡å°‘tokené™åˆ¶
        self.unlimited_retries = True  # æ— é™åˆ¶é‡è¯•
        self.deep_analysis_mode = True   # å¯ç”¨æ·±åº¦åˆ†ææ¨¡å¼
        self.simplified_mode = False    # å…³é—­ç®€åŒ–æ¨¡å¼

        # APIå¯†é’¥åˆå§‹åŒ–
        self.api_key = self._init_llm_client(api_key)

        # æ•°æ®å­˜å‚¨
        self.discovered_files = {}  # å‘ç°çš„æ‰€æœ‰æ–‡ä»¶
        self.llm_file_analysis = {}  # LLMæ–‡ä»¶åˆ†æç»“æœ
        self.llm_schema_design = {}  # LLMæ•°æ®åº“è®¾è®¡æ–¹æ¡ˆ
        self.llm_business_intelligence = {}  # LLMä¸šåŠ¡æ™ºèƒ½åˆ†æ
        self.import_execution_log = []  # è¯¦ç»†æ‰§è¡Œæ—¥å¿—

        logger.info("ğŸ§  çº¯LLMæ™ºèƒ½æ•°æ®å¯¼å…¥ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def _init_llm_client(self, api_key: Optional[str] = None) -> Optional[str]:
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ - ä½¿ç”¨ç›´æ¥requestsé¿å…LangChainé—®é¢˜"""
        try:
            if not api_key:
                api_key = os.getenv('DEEPSEEK_API_KEY')

            if not api_key:
                logger.error("âŒ æœªæ‰¾åˆ°LLM APIå¯†é’¥ï¼Œçº¯LLMç³»ç»Ÿæ— æ³•è¿è¡Œ")
                raise ValueError("çº¯LLMç³»ç»Ÿéœ€è¦APIå¯†é’¥æ‰èƒ½è¿è¡Œ")

            # ç›´æ¥å­˜å‚¨APIå¯†é’¥ï¼Œä¸ä½¿ç”¨LangChain
            logger.info("ğŸ”§ ä½¿ç”¨ç›´æ¥requests APIè°ƒç”¨ï¼Œé¿å…LangChainé—®é¢˜")
            logger.info("âœ… APIå¯†é’¥é…ç½®æˆåŠŸ - ç›´æ¥requestsæ¨¡å¼")

            return api_key

        except Exception as e:
            logger.error(f"âŒ APIå¯†é’¥é…ç½®å¤±è´¥: {e}")
            raise

    def process_batch_import(self, file_paths: List[str], output_db_path: str) -> Dict[str, Any]:
        """
        çº¯LLMé©±åŠ¨çš„æ‰¹é‡å¯¼å…¥ä¸»æµç¨‹

        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
            output_db_path: è¾“å‡ºæ•°æ®åº“è·¯å¾„

        Returns:
            è¯¦ç»†çš„å¤„ç†æŠ¥å‘Š
        """
        logger.info("ğŸ§  å¼€å§‹çº¯LLMæ™ºèƒ½æ•°æ®å¯¼å…¥å¤„ç†")
        start_time = datetime.now()

        try:
            # ç¬¬1æ­¥: LLMé©±åŠ¨çš„æ–‡ä»¶å‘ç°å’Œåˆ†ç±»
            self._llm_discover_and_classify_files(file_paths)

            # ç¬¬2æ­¥: LLMæ·±åº¦æ–‡ä»¶å†…å®¹åˆ†æ
            self._llm_deep_content_analysis()

            # ç¬¬3æ­¥: LLMæ™ºèƒ½æ•°æ®åº“è®¾è®¡
            self._llm_intelligent_database_design()

            # ç¬¬4æ­¥: LLMæŒ‡å¯¼çš„æ•°æ®åº“åˆ›å»º
            self._llm_guided_database_creation(output_db_path)

            # ç¬¬5æ­¥: LLMæ™ºèƒ½æ•°æ®å¯¼å…¥
            self._llm_intelligent_data_import(output_db_path)

            # ç¬¬6æ­¥: LLMä¸šåŠ¡æ™ºèƒ½åˆ†æ
            self._llm_business_intelligence_analysis()

            # ç¬¬7æ­¥: LLMç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            report = self._llm_generate_comprehensive_report(output_db_path, start_time)

            logger.info("âœ… çº¯LLMæ™ºèƒ½æ•°æ®å¯¼å…¥å®Œæˆ")
            return report

        except Exception as e:
            logger.error(f"âŒ çº¯LLMæ™ºèƒ½æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'execution_time': (datetime.now() - start_time).total_seconds()
            }

    def _llm_discover_and_classify_files(self, file_paths: List[str]):
        """
        ç¬¬1æ­¥: LLMé©±åŠ¨çš„æ–‡ä»¶å‘ç°å’Œåˆ†ç±»
        å®Œå…¨ä¾èµ–LLMåˆ†ææ–‡ä»¶åå’Œå†…å®¹æ¥åˆ†ç±»æ–‡ä»¶
        """
        logger.info("ğŸ” LLMæ–‡ä»¶å‘ç°å’Œåˆ†ç±»åˆ†æ")

        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯
        for file_path in file_paths:
            if os.path.exists(file_path) and file_path.endswith(('.xlsx', '.xls', '.csv')):
                try:
                    # è¯»å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
                    file_info = self._extract_file_basic_info(file_path)
                    self.discovered_files[file_path] = file_info

                    logger.info(f"ğŸ“„ å‘ç°æ–‡ä»¶: {os.path.basename(file_path)}")

                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        # ä½¿ç”¨LLMåˆ†æå’Œåˆ†ç±»æ‰€æœ‰æ–‡ä»¶
        if self.discovered_files:
            self._llm_classify_files_by_content()

    def _extract_file_basic_info(self, file_path: str) -> Dict[str, Any]:
        """æå–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯ä¾›LLMåˆ†æ"""
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path, nrows=20)  # è¯»å–æ›´å¤šè¡Œä¾›LLMåˆ†æ
            else:
                df = pd.read_excel(file_path, nrows=20)

            # æå–è¯¦ç»†ä¿¡æ¯ä¾›LLMåˆ†æ
            file_info = {
                'file_path': file_path,
                'file_name': os.path.basename(file_path),
                'file_size': os.path.getsize(file_path),
                'columns': list(df.columns),
                'column_count': len(df.columns),
                'sample_rows': df.head(10).to_dict('records'),
                'data_types': {col: str(dtype) for col, dtype in df.dtypes.items()},
                'null_counts': df.isnull().sum().to_dict(),
                'unique_counts': df.nunique().to_dict(),
                'sample_values_per_column': {
                    col: df[col].dropna().unique()[:5].tolist()
                    for col in df.columns
                }
            }

            return file_info

        except Exception as e:
            logger.error(f"âŒ æå–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")
            return {}

    def _llm_classify_files_by_content(self):
        """ä½¿ç”¨LLMåˆ†ææ–‡ä»¶å†…å®¹è¿›è¡Œæ™ºèƒ½åˆ†ç±»"""
        logger.info("ğŸ§  LLMæ™ºèƒ½æ–‡ä»¶åˆ†ç±»åˆ†æ")

        # æ„å»ºæ–‡ä»¶åˆ†ç±»åˆ†ææç¤ºè¯
        classification_prompt = self._build_file_classification_prompt()

        # è°ƒç”¨LLMè¿›è¡Œæ–‡ä»¶åˆ†ç±»
        classification_result = self._call_llm_unlimited_retry(classification_prompt)

        if classification_result:
            try:
                # è§£æLLMåˆ†ç±»ç»“æœ
                parsed_result = self._parse_llm_json_response(classification_result)

                if parsed_result:
                    # æ›´æ–°æ–‡ä»¶åˆ†æç»“æœ
                    for file_path, file_info in self.discovered_files.items():
                        file_name = os.path.basename(file_path)

                        # æŸ¥æ‰¾LLMå¯¹è¯¥æ–‡ä»¶çš„åˆ†ç±»ç»“æœ
                        for file_analysis in parsed_result.get('file_classifications', []):
                            if file_analysis.get('file_name') == file_name:
                                file_info['llm_classification'] = file_analysis
                                logger.info(f"ğŸ“‹ LLMåˆ†ç±»: {file_name} -> {file_analysis.get('file_type', 'unknown')}")
                                break

            except Exception as e:
                logger.error(f"âŒ LLMæ–‡ä»¶åˆ†ç±»ç»“æœè§£æå¤±è´¥: {e}")

    def _build_file_classification_prompt(self) -> str:
        """æ„å»ºæ–‡ä»¶åˆ†ç±»åˆ†ææç¤ºè¯"""

        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
        files_info = []
        for file_path, file_info in self.discovered_files.items():
            # è½¬æ¢æ ·æœ¬æ•°æ®ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
            sample_data = []
            for row in file_info['sample_rows'][:3]:  # å‰3è¡Œæ ·æœ¬
                if isinstance(row, dict):
                    # è½¬æ¢å­—å…¸ä¸­çš„å€¼ä¸ºå­—ç¬¦ä¸²
                    converted_row = {}
                    for key, value in row.items():
                        if hasattr(value, 'strftime'):  # æ—¥æœŸæ—¶é—´å¯¹è±¡
                            converted_row[key] = str(value)
                        elif hasattr(value, 'item'):  # numpyå¯¹è±¡
                            converted_row[key] = value.item()
                        else:
                            converted_row[key] = str(value)
                    sample_data.append(converted_row)
                else:
                    sample_data.append(str(row))

            files_info.append({
                'file_name': file_info['file_name'],
                'columns': file_info['columns'],
                'sample_data': sample_data,
                'data_types': [str(dt) for dt in file_info['data_types']],  # è½¬æ¢æ•°æ®ç±»å‹ä¸ºå­—ç¬¦ä¸²
                'column_count': file_info['column_count']
            })

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹æ–‡ä»¶å¹¶è¿›è¡Œæ™ºèƒ½åˆ†ç±»ã€‚

## å¾…åˆ†ææ–‡ä»¶ä¿¡æ¯
{json.dumps(files_info, ensure_ascii=False, indent=2)}

## åˆ†æä»»åŠ¡
è¯·å¯¹æ¯ä¸ªæ–‡ä»¶è¿›è¡Œæ·±åº¦åˆ†æï¼Œåˆ¤æ–­å…¶ç±»å‹å’Œç”¨é€”ã€‚ä¸è¦ä½¿ç”¨ä»»ä½•é¢„å®šä¹‰çš„è§„åˆ™æˆ–å…³é”®è¯åŒ¹é…ï¼Œå®Œå…¨åŸºäºæ–‡ä»¶å†…å®¹çš„è¯­ä¹‰ç†è§£è¿›è¡Œåˆ†æã€‚

## åˆ†æç»´åº¦
1. **æ–‡ä»¶ç±»å‹è¯†åˆ«**ï¼š
   - ä¸šåŠ¡æ•°æ®æ–‡ä»¶ï¼šåŒ…å«å®é™…ä¸šåŠ¡æ•°æ®çš„æ–‡ä»¶
   - æ•°æ®å­—å…¸æ–‡ä»¶ï¼šå®šä¹‰å­—æ®µç»“æ„å’Œå«ä¹‰çš„æ–‡ä»¶
   - é…ç½®æ–‡ä»¶ï¼šåŒ…å«ç³»ç»Ÿé…ç½®ä¿¡æ¯çš„æ–‡ä»¶
   - å…¶ä»–ç±»å‹ï¼šè¯·å…·ä½“è¯´æ˜

2. **ä¸šåŠ¡é¢†åŸŸè¯†åˆ«**ï¼š
   - åˆ†ææ–‡ä»¶æ¶‰åŠçš„ä¸šåŠ¡é¢†åŸŸï¼ˆå¦‚é“¶è¡Œã€ä¿é™©ã€ç”µå•†ç­‰ï¼‰
   - è¯†åˆ«å…·ä½“çš„ä¸šåŠ¡åœºæ™¯ï¼ˆå¦‚å®¢æˆ·ç®¡ç†ã€é£é™©æ§åˆ¶ã€è´¢åŠ¡åˆ†æç­‰ï¼‰

3. **æ•°æ®ç‰¹å¾åˆ†æ**ï¼š
   - æ•°æ®çš„ä¸»è¦ç‰¹å¾å’Œæ¨¡å¼
   - å­—æ®µçš„ä¸šåŠ¡å«ä¹‰æ¨æ–­
   - æ•°æ®è´¨é‡è¯„ä¼°

4. **å…³è”å…³ç³»åˆ†æ**ï¼š
   - æ–‡ä»¶ä¹‹é—´å¯èƒ½çš„å…³è”å…³ç³»
   - ä¸»å¤–é”®å…³ç³»æ¨æ–­
   - ä¸šåŠ¡æµç¨‹å…³ç³»

## è¾“å‡ºæ ¼å¼
è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

{{
  "analysis_summary": "æ•´ä½“åˆ†ææ€»ç»“",
  "business_domain": "è¯†åˆ«çš„ä¸šåŠ¡é¢†åŸŸ",
  "file_classifications": [
    {{
      "file_name": "æ–‡ä»¶å",
      "file_type": "æ–‡ä»¶ç±»å‹",
      "business_purpose": "ä¸šåŠ¡ç”¨é€”",
      "data_characteristics": "æ•°æ®ç‰¹å¾æè¿°",
      "key_fields": ["å…³é”®å­—æ®µåˆ—è¡¨"],
      "business_concepts": ["è¯†åˆ«çš„ä¸šåŠ¡æ¦‚å¿µ"],
      "data_quality_assessment": "æ•°æ®è´¨é‡è¯„ä¼°",
      "relationships_with_other_files": "ä¸å…¶ä»–æ–‡ä»¶çš„å…³ç³»",
      "confidence_score": 0.95
    }}
  ],
  "overall_relationships": [
    {{
      "file1": "æ–‡ä»¶1",
      "file2": "æ–‡ä»¶2",
      "relationship_type": "å…³ç³»ç±»å‹",
      "relationship_description": "å…³ç³»æè¿°",
      "confidence": 0.9
    }}
  ],
  "business_intelligence_insights": [
    "ä¸šåŠ¡æ™ºèƒ½æ´å¯Ÿ1",
    "ä¸šåŠ¡æ™ºèƒ½æ´å¯Ÿ2"
  ]
}}

è¯·è¿›è¡Œæ·±åº¦åˆ†æï¼Œä¸è¦ä½¿ç”¨ä»»ä½•é¢„è®¾çš„è§„åˆ™æˆ–æ¨¡å¼åŒ¹é…ï¼Œå®Œå…¨åŸºäºå¯¹æ•°æ®å†…å®¹çš„ç†è§£æ¥è¿›è¡Œåˆ†ç±»å’Œåˆ†æã€‚
"""

        return prompt

    def _call_llm_unlimited_retry(self, prompt: str) -> Optional[str]:
        """ç›´æ¥requestsè°ƒç”¨ - é¿å…LangChainé—®é¢˜"""
        max_attempts = 15
        attempt = 0

        while attempt < max_attempts:
            attempt += 1
            try:
                logger.info(f"ğŸ¤– ç›´æ¥APIè°ƒç”¨ (ç¬¬ {attempt} æ¬¡å°è¯•)")

                # æ ¹æ®å°è¯•æ¬¡æ•°è°ƒæ•´è¯·æ±‚å‚æ•° - é’ˆå¯¹ChunkedEncodingErrorä¼˜åŒ–
                if attempt > 3:
                    adjusted_tokens = max(1000, self.max_tokens_per_request // 2)
                    logger.info(f"ğŸ”§ è°ƒæ•´max_tokensä¸º {adjusted_tokens} (é¿å…ChunkedEncodingError)")
                elif attempt > 1:
                    adjusted_tokens = max(2000, self.max_tokens_per_request * 3 // 4)
                    logger.info(f"ğŸ”§ è°ƒæ•´max_tokensä¸º {adjusted_tokens} (å‡å°‘å“åº”å¤§å°)")
                else:
                    adjusted_tokens = self.max_tokens_per_request

                # æ„å»ºè¯·æ±‚æ•°æ®
                headers = {
                    'Authorization': f'Bearer {self.api_key}',
                    'Content-Type': 'application/json'
                }

                data = {
                    'model': 'deepseek-chat',
                    'messages': [
                        {
                            'role': 'system',
                            'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå’Œæ•°æ®åº“è®¾è®¡ä¸“å®¶ï¼Œæ“…é•¿é€šè¿‡æ·±åº¦åˆ†æç†è§£æ•°æ®çš„ä¸šåŠ¡å«ä¹‰å’Œç»“æ„å…³ç³»ã€‚'
                        },
                        {
                            'role': 'user',
                            'content': prompt
                        }
                    ],
                    'max_tokens': adjusted_tokens,
                    'temperature': 0.1
                }

                # å‘é€è¯·æ±‚
                response = requests.post(
                    'https://api.deepseek.com/chat/completions',
                    headers=headers,
                    json=data,
                    timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
                )

                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    logger.info(f"âœ… APIè°ƒç”¨æˆåŠŸ (ç¬¬ {attempt} æ¬¡å°è¯•)")
                    return content.strip()
                else:
                    raise Exception(f"APIé”™è¯¯: {response.status_code} - {response.text}")

            except Exception as e:
                error_type = type(e).__name__
                error_msg = str(e)

                logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ (ç¬¬ {attempt} æ¬¡): {error_type} - {error_msg}")

                # æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´é‡è¯•ç­–ç•¥
                if "ChunkedEncodingError" in error_type or "prematurely" in error_msg:
                    wait_time = min(5 + (attempt * 3), 30)
                    logger.info(f"ğŸ“¦ å“åº”ä¼ è¾“ä¸­æ–­ï¼Œç­‰å¾… {wait_time} ç§’")
                elif "ConnectionError" in error_type or "timeout" in error_msg.lower():
                    wait_time = min(10 + (attempt * 5), 60)
                    logger.info(f"ğŸŒ ç½‘ç»œé—®é¢˜ï¼Œç­‰å¾… {wait_time} ç§’")
                elif "429" in error_msg or "rate" in error_msg.lower():
                    wait_time = min(30 + (attempt * 10), 120)
                    logger.info(f"ğŸš¦ APIé™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’")
                else:
                    wait_time = min(2 ** attempt, 60)
                    logger.info(f"â“ å…¶ä»–é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’")

                if attempt < max_attempts:
                    logger.info(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"âŒ APIè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œå·²å°è¯• {max_attempts} æ¬¡")
                    return None

        return None

    def _parse_llm_json_response(self, response: str) -> Optional[Dict[str, Any]]:
        """è§£æLLMçš„JSONå“åº”ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        if not response or not response.strip():
            logger.warning("âš ï¸ å“åº”ä¸ºç©º")
            return None

        # æ¸…ç†å“åº”å†…å®¹
        cleaned_response = self._clean_llm_response(response)

        try:
            # å°è¯•ç›´æ¥è§£æ
            parsed_result = json.loads(cleaned_response)
            logger.info("âœ… LLMå“åº”è§£ææˆåŠŸ")
            return parsed_result

        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
            logger.error(f"ğŸ“ åŸå§‹å“åº”å‰100å­—ç¬¦: {response[:100]}...")
            logger.error(f"ğŸ§¹ æ¸…ç†åå“åº”å‰100å­—ç¬¦: {cleaned_response[:100]}...")
            return None
        except Exception as e:
            logger.error(f"âŒ å“åº”è§£æå¤±è´¥: {e}")
            return None

    def _clean_llm_response(self, response: str) -> str:
        """æ¸…ç†LLMå“åº”ï¼Œæå–JSONéƒ¨åˆ†"""
        if not response:
            return ""

        # ç§»é™¤å¯èƒ½çš„å‰ç¼€æ–‡æœ¬
        response = response.strip()

        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        if response.startswith('```json'):
            response = response[7:]
        elif response.startswith('```'):
            response = response[3:]

        if response.endswith('```'):
            response = response[:-3]

        response = response.strip()

        # æŸ¥æ‰¾JSONå¼€å§‹ä½ç½®
        json_start = response.find('{')
        if json_start == -1:
            # æ²¡æœ‰æ‰¾åˆ°JSONï¼Œè¿”å›åŸå§‹å“åº”
            return response

        # æŸ¥æ‰¾JSONç»“æŸä½ç½®
        brace_count = 0
        json_end = -1

        for i in range(json_start, len(response)):
            if response[i] == '{':
                brace_count += 1
            elif response[i] == '}':
                brace_count -= 1
                if brace_count == 0:
                    json_end = i + 1
                    break

        if json_end == -1:
            # æ²¡æœ‰æ‰¾åˆ°å®Œæ•´çš„JSONï¼Œè¿”å›ä»å¼€å§‹ä½ç½®åˆ°ç»“å°¾
            return response[json_start:]

        # è¿”å›æå–çš„JSONéƒ¨åˆ†
        return response[json_start:json_end]

    def _extract_schema_info(self, file_path: str) -> Dict[str, Any]:
        """æå–æ–‡ä»¶çš„schemaä¿¡æ¯ï¼ˆå­—æ®µåã€ç±»å‹ã€æ ·æœ¬å€¼ï¼‰"""
        try:
            df = pd.read_excel(file_path)

            schema_info = {
                'file_name': os.path.basename(file_path),
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'fields': []
            }

            for col in df.columns:
                # åˆ†æå­—æ®µç±»å‹
                col_data = df[col].dropna()
                if len(col_data) == 0:
                    data_type = 'TEXT'
                    sample_values = []
                else:
                    # æ™ºèƒ½ç±»å‹æ£€æµ‹
                    if col_data.dtype in ['int64', 'int32']:
                        data_type = 'INTEGER'
                    elif col_data.dtype in ['float64', 'float32']:
                        data_type = 'REAL'
                    elif pd.api.types.is_datetime64_any_dtype(col_data):
                        data_type = 'DATE'
                    else:
                        data_type = 'TEXT'

                    # è·å–æ ·æœ¬å€¼ï¼ˆæœ€å¤š3ä¸ªï¼‰
                    sample_values = col_data.head(3).tolist()
                    # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
                    sample_values = [str(v) if not isinstance(v, (int, float, str, bool)) else v for v in sample_values]

                schema_info['fields'].append({
                    'field_name': col,
                    'detected_type': data_type,
                    'sample_values': sample_values,
                    'null_count': df[col].isna().sum(),
                    'unique_count': df[col].nunique()
                })

            return schema_info

        except Exception as e:
            logger.error(f"âŒ Schemaæå–å¤±è´¥ {file_path}: {e}")
            return None

    def _convert_data_dict_to_markdown(self, file_path: str) -> str:
        """å°†æ•°æ®å­—å…¸æ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        try:
            df = pd.read_excel(file_path)

            markdown_content = f"# æ•°æ®å­—å…¸: {os.path.basename(file_path)}\n\n"

            # è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼
            if len(df.columns) >= 2:
                markdown_content += "| " + " | ".join(df.columns) + " |\n"
                markdown_content += "| " + " | ".join(["---"] * len(df.columns)) + " |\n"

                for _, row in df.iterrows():
                    row_values = [str(v) if not pd.isna(v) else "" for v in row]
                    markdown_content += "| " + " | ".join(row_values) + " |\n"

            return markdown_content

        except Exception as e:
            logger.error(f"âŒ æ•°æ®å­—å…¸è½¬æ¢å¤±è´¥ {file_path}: {e}")
            return f"æ•°æ®å­—å…¸æ–‡ä»¶: {os.path.basename(file_path)} (è½¬æ¢å¤±è´¥)"

    def _llm_deep_content_analysis(self):
        """
        ç®€åŒ–ç­–ç•¥ï¼šåŸºäºè§„åˆ™çš„å¿«é€Ÿå¯¼å…¥ + æœ€å°LLMå¢å¼º
        """
        logger.info("ğŸ”¬ ç®€åŒ–ç­–ç•¥åˆ†æ")

        # ç›´æ¥å¤„ç†æ¯ä¸ªä¸šåŠ¡æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨åŸºç¡€schema + ç®€å•LLMå¢å¼º
        for file_path, file_info in self.discovered_files.items():
            if file_info.get('classification') == 'ä¸šåŠ¡æ•°æ®æ–‡ä»¶':
                logger.info(f"ï¿½ å¤„ç†æ–‡ä»¶: {file_info['file_name']}")

                try:
                    # è¯»å–Excelæ–‡ä»¶
                    df = pd.read_excel(file_path)

                    # æ„å»ºåŸºç¡€åˆ†æç»“æœ
                    basic_analysis = {
                        'file_name': file_info['file_name'],
                        'table_name_suggestion': self._generate_table_name(file_info['file_name']),
                        'total_rows': len(df),
                        'total_columns': len(df.columns),
                        'columns': list(df.columns),
                        'field_analysis': []
                    }

                    # åˆ†ææ¯ä¸ªå­—æ®µ
                    for col in df.columns:
                        col_data = df[col].dropna()

                        # æ™ºèƒ½ç±»å‹æ£€æµ‹
                        if len(col_data) == 0:
                            data_type = 'TEXT'
                        elif col_data.dtype in ['int64', 'int32']:
                            data_type = 'INTEGER'
                        elif col_data.dtype in ['float64', 'float32']:
                            data_type = 'REAL'
                        elif pd.api.types.is_datetime64_any_dtype(col_data):
                            data_type = 'DATE'
                        else:
                            data_type = 'TEXT'

                        # è·å–æ ·æœ¬å€¼
                        sample_values = col_data.head(3).tolist() if len(col_data) > 0 else []

                        basic_analysis['field_analysis'].append({
                            'field_name': col,
                            'english_name': self._generate_english_name(col),
                            'data_type': data_type,
                            'business_meaning': self._guess_business_meaning(col),
                            'sample_values': sample_values,
                            'is_primary_key': self._is_likely_primary_key(col, col_data),
                            'is_foreign_key': False
                        })

                    # å­˜å‚¨åˆ†æç»“æœ
                    self.llm_file_analysis[file_path] = {
                        'basic_info': basic_analysis,
                        'llm_analysis': basic_analysis,  # ä½¿ç”¨åŸºç¡€åˆ†æä½œä¸ºLLMåˆ†æ
                        'sample_data': df.head(3).to_dict('records')
                    }

                    logger.info(f"âœ… åˆ†æå®Œæˆ: {file_info['file_name']} ({len(df.columns)}ä¸ªå­—æ®µ)")

                except Exception as e:
                    logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {file_info['file_name']} - {e}")
                    continue

    def _generate_table_name(self, file_name: str) -> str:
        """ç”Ÿæˆè¡¨å"""
        # ç§»é™¤æ‰©å±•åå’Œç‰¹æ®Šå­—ç¬¦
        name = file_name.replace('.xlsx', '').replace('.xls', '')
        # ç®€å•çš„æ˜ å°„è§„åˆ™
        if 'åˆåŒ' in name and 'åˆ†ç±»' in name:
            return 'contract_classification'
        elif 'è´·æ¬¾' in name and 'åˆåŒ' in name:
            return 'loan_contract_info'
        elif 'å­˜æ¬¾' in name and 'ä½™é¢' in name:
            return 'deposit_balance'
        else:
            # é»˜è®¤ä½¿ç”¨æ–‡ä»¶åçš„æ‹¼éŸ³æˆ–ç®€åŒ–ç‰ˆæœ¬
            return 'data_table'

    def _generate_english_name(self, chinese_name: str) -> str:
        """ç”Ÿæˆè‹±æ–‡å­—æ®µå"""
        # ç®€å•çš„æ˜ å°„è§„åˆ™
        mapping = {
            'å®¢æˆ·å·': 'customer_id',
            'å®¢æˆ·åç§°': 'customer_name',
            'åˆåŒå·': 'contract_id',
            'è´·æ¬¾ä½™é¢': 'loan_balance',
            'å­˜æ¬¾ä½™é¢': 'deposit_balance',
            'æ—¥æœŸ': 'date',
            'é‡‘é¢': 'amount',
            'åˆ©ç‡': 'interest_rate',
            'æœŸé™': 'term',
            'çŠ¶æ€': 'status'
        }
        return mapping.get(chinese_name, chinese_name.lower().replace(' ', '_'))

    def _guess_business_meaning(self, field_name: str) -> str:
        """çŒœæµ‹ä¸šåŠ¡å«ä¹‰"""
        if 'å®¢æˆ·' in field_name:
            return 'å®¢æˆ·ç›¸å…³ä¿¡æ¯'
        elif 'åˆåŒ' in field_name:
            return 'åˆåŒç›¸å…³ä¿¡æ¯'
        elif 'ä½™é¢' in field_name:
            return 'é‡‘é¢ä½™é¢'
        elif 'æ—¥æœŸ' in field_name:
            return 'æ—¥æœŸæ—¶é—´'
        else:
            return 'ä¸šåŠ¡æ•°æ®å­—æ®µ'

    def _is_likely_primary_key(self, field_name: str, data) -> bool:
        """åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯ä¸»é”®"""
        if len(data) == 0:
            return False
        # å¦‚æœå­—æ®µååŒ…å«IDæˆ–å”¯ä¸€å€¼æ¯”ä¾‹å¾ˆé«˜
        if 'id' in field_name.lower() or 'å·' in field_name:
            unique_ratio = len(data.unique()) / len(data)
            return unique_ratio > 0.9
        return False

    def _build_deep_analysis_prompt(self, file_info: Dict[str, Any]) -> str:
        """æ„å»ºæ·±åº¦åˆ†ææç¤ºè¯"""

        # ç®€åŒ–æ¨¡å¼å·²å…³é—­ï¼Œä½¿ç”¨å®Œæ•´åˆ†æ

        # è½¬æ¢æ ·æœ¬æ•°æ®ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼ - å‡å°‘æ•°æ®é‡é¿å…ChunkedEncodingError
        sample_rows_serializable = []
        # å¤§å¹…å‡å°‘æ ·æœ¬è¡Œæ•°
        max_sample_rows = 3
        for row in file_info['sample_rows'][:max_sample_rows]:
            if isinstance(row, dict):
                converted_row = {}
                for key, value in row.items():
                    if hasattr(value, 'strftime'):  # æ—¥æœŸæ—¶é—´å¯¹è±¡
                        converted_row[key] = str(value)
                    elif hasattr(value, 'item'):  # numpyå¯¹è±¡
                        converted_row[key] = value.item()
                    else:
                        # é™åˆ¶å­—ç¬¦ä¸²é•¿åº¦ä»¥å‡å°‘æ•°æ®é‡
                        str_value = str(value)
                        if len(str_value) > 100:
                            str_value = str_value[:100] + "..."
                        converted_row[key] = str_value
                sample_rows_serializable.append(converted_row)
            else:
                sample_rows_serializable.append(str(row))

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„æ•°æ®æ¶æ„å¸ˆå’Œä¸šåŠ¡åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹æ–‡ä»¶è¿›è¡Œæ·±åº¦çš„ä¸šåŠ¡é€»è¾‘å’Œæ•°æ®ç»“æ„åˆ†æã€‚

## æ–‡ä»¶ä¿¡æ¯
æ–‡ä»¶å: {file_info['file_name']}
å­—æ®µæ•°é‡: {file_info['column_count']}
æ–‡ä»¶å¤§å°: {file_info['file_size']} å­—èŠ‚

## å­—æ®µåˆ—è¡¨
{json.dumps(file_info['columns'], ensure_ascii=False, indent=2)}

## æ•°æ®ç±»å‹ä¿¡æ¯
{json.dumps(file_info['data_types'], ensure_ascii=False, indent=2)}

## æ ·æœ¬æ•°æ® (å‰3è¡Œï¼Œä¼˜åŒ–å¤§å°)
{json.dumps(sample_rows_serializable, ensure_ascii=False, indent=2)}

## æ•°æ®ç»Ÿè®¡ä¿¡æ¯
ç©ºå€¼ç»Ÿè®¡: {json.dumps(file_info['null_counts'], ensure_ascii=False, indent=2)}
å”¯ä¸€å€¼ç»Ÿè®¡: {json.dumps(file_info['unique_counts'], ensure_ascii=False, indent=2)}

## æ¯åˆ—æ ·æœ¬å€¼ (é™åˆ¶æ•°é‡)
{json.dumps(self._convert_to_serializable_limited(file_info['sample_values_per_column']), ensure_ascii=False, indent=2)}

## æ·±åº¦åˆ†æä»»åŠ¡
è¯·è¿›è¡Œä»¥ä¸‹ç»´åº¦çš„æ·±åº¦åˆ†æï¼Œä¸è¦ä½¿ç”¨ä»»ä½•é¢„è®¾è§„åˆ™ï¼Œå®Œå…¨åŸºäºæ•°æ®å†…å®¹çš„è¯­ä¹‰ç†è§£ï¼š

1. **ä¸šåŠ¡è¯­ä¹‰åˆ†æ**ï¼š
   - æ¯ä¸ªå­—æ®µçš„çœŸå®ä¸šåŠ¡å«ä¹‰
   - å­—æ®µä¹‹é—´çš„ä¸šåŠ¡é€»è¾‘å…³ç³»
   - æ•°æ®åæ˜ çš„ä¸šåŠ¡æµç¨‹å’Œåœºæ™¯

2. **æ•°æ®ç»“æ„åˆ†æ**ï¼š
   - ä¸»é”®å­—æ®µè¯†åˆ«åŠç†ç”±
   - å¤–é”®å…³ç³»æ¨æ–­
   - æ•°æ®å®Œæ•´æ€§çº¦æŸå»ºè®®

3. **æ•°æ®è´¨é‡åˆ†æ**ï¼š
   - æ•°æ®è´¨é‡é—®é¢˜è¯†åˆ«
   - æ•°æ®æ¸…æ´—å»ºè®®
   - å¼‚å¸¸å€¼å’Œç¼ºå¤±å€¼å¤„ç†ç­–ç•¥

4. **ä¸šåŠ¡è§„åˆ™æŒ–æ˜**ï¼š
   - ä»æ•°æ®ä¸­å‘ç°çš„ä¸šåŠ¡è§„åˆ™
   - æ•°æ®éªŒè¯è§„åˆ™å»ºè®®
   - ä¸šåŠ¡æœ¯è¯­å®šä¹‰

5. **æ ‡å‡†åŒ–å»ºè®®**ï¼š
   - å­—æ®µå‘½åæ ‡å‡†åŒ–å»ºè®®
   - æ•°æ®ç±»å‹ä¼˜åŒ–å»ºè®®
   - ç´¢å¼•ç­–ç•¥å»ºè®®

## è¾“å‡ºæ ¼å¼
è¯·ä»¥JSONæ ¼å¼è¿”å›è¯¦ç»†åˆ†æç»“æœï¼š

{{
  "file_analysis": {{
    "business_domain": "ä¸šåŠ¡é¢†åŸŸ",
    "business_scenario": "å…·ä½“ä¸šåŠ¡åœºæ™¯",
    "data_purpose": "æ•°æ®ç”¨é€”æè¿°"
  }},
  "field_analysis": [
    {{
      "field_name": "å­—æ®µå",
      "business_meaning": "ä¸šåŠ¡å«ä¹‰",
      "data_type_recommendation": "æ¨èæ•°æ®ç±»å‹",
      "is_primary_key": true/false,
      "is_foreign_key": true/false,
      "foreign_key_reference": "å¤–é”®å¼•ç”¨è¡¨.å­—æ®µ",
      "is_required": true/false,
      "business_rules": ["ä¸šåŠ¡è§„åˆ™1", "ä¸šåŠ¡è§„åˆ™2"],
      "data_quality_issues": ["è´¨é‡é—®é¢˜1", "è´¨é‡é—®é¢˜2"],
      "standardized_name": "æ ‡å‡†åŒ–å­—æ®µå",
      "validation_rules": ["éªŒè¯è§„åˆ™1", "éªŒè¯è§„åˆ™2"],
      "index_recommendation": "ç´¢å¼•å»ºè®®"
    }}
  ],
  "business_terms": [
    {{
      "term": "ä¸šåŠ¡æœ¯è¯­",
      "definition": "æœ¯è¯­å®šä¹‰",
      "sql_expression": "SQLè¡¨è¾¾å¼",
      "applicable_fields": ["é€‚ç”¨å­—æ®µåˆ—è¡¨"]
    }}
  ],
  "data_relationships": [
    {{
      "relationship_type": "å…³ç³»ç±»å‹",
      "description": "å…³ç³»æè¿°",
      "fields_involved": ["æ¶‰åŠå­—æ®µ"]
    }}
  ],
  "table_design": {{
    "recommended_table_name": "æ¨èè¡¨å",
    "table_description": "è¡¨æè¿°",
    "primary_key_fields": ["ä¸»é”®å­—æ®µ"],
    "indexes": [
      {{
        "index_name": "ç´¢å¼•å",
        "fields": ["å­—æ®µåˆ—è¡¨"],
        "index_type": "ç´¢å¼•ç±»å‹"
      }}
    ]
  }},
  "data_quality_report": {{
    "overall_quality_score": 0.85,
    "quality_issues": ["é—®é¢˜åˆ—è¡¨"],
    "cleaning_recommendations": ["æ¸…æ´—å»ºè®®"],
    "validation_suggestions": ["éªŒè¯å»ºè®®"]
  }},
  "business_intelligence": {{
    "key_insights": ["å…³é”®æ´å¯Ÿ"],
    "business_value": "ä¸šåŠ¡ä»·å€¼",
    "usage_scenarios": ["ä½¿ç”¨åœºæ™¯"]
  }}
}}

è¯·åŸºäºå¯¹æ•°æ®çš„æ·±åº¦ç†è§£è¿›è¡Œåˆ†æï¼Œä¸è¦ä½¿ç”¨ä»»ä½•é¢„è®¾çš„æ¨¡å¼æˆ–è§„åˆ™ã€‚
"""

        return prompt

    def _build_simplified_analysis_prompt(self, file_info: Dict[str, Any]) -> str:
        """æ„å»ºç®€åŒ–åˆ†ææç¤ºè¯ - æåº¦å‹ç¼©ç‰ˆæœ¬"""

        # åªå–æœ€åŸºç¡€çš„ä¿¡æ¯
        basic_info = {
            'file_name': file_info['file_name'],
            'columns': file_info['columns'][:10],  # æœ€å¤š10ä¸ªå­—æ®µ
            'column_count': min(file_info['column_count'], 10),
            'row_count': file_info['row_count']
        }

        prompt = f"""ä½ æ˜¯æ•°æ®åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹æ–‡ä»¶è¿›è¡Œå¿«é€Ÿåˆ†æã€‚

æ–‡ä»¶å: {basic_info['file_name']}
å­—æ®µæ•°: {basic_info['column_count']}
æ•°æ®è¡Œæ•°: {basic_info['row_count']}
å­—æ®µåˆ—è¡¨: {', '.join(basic_info['columns'])}

è¯·è¿”å›JSONæ ¼å¼çš„ç®€åŒ–åˆ†æï¼š
{{
  "file_analysis": {{
    "business_domain": "æ ¹æ®æ–‡ä»¶åæ¨æ–­çš„ä¸šåŠ¡é¢†åŸŸ",
    "table_name": "å»ºè®®çš„æ•°æ®åº“è¡¨å",
    "data_purpose": "æ•°æ®ç”¨é€”ç®€è¿°"
  }},
  "field_mapping": {{
    "primary_fields": ["ä¸»è¦å­—æ®µ1", "ä¸»è¦å­—æ®µ2"],
    "data_types": {{"å­—æ®µå": "æ¨æ–­ç±»å‹"}}
  }}
}}

è¦æ±‚ï¼š
1. åˆ†æè¦ç®€æ´å‡†ç¡®
2. è¡¨åä½¿ç”¨è‹±æ–‡ï¼Œç¬¦åˆæ•°æ®åº“å‘½åè§„èŒƒ
3. é‡ç‚¹å…³æ³¨æ ¸å¿ƒä¸šåŠ¡å­—æ®µ
4. å“åº”æ§åˆ¶åœ¨500å­—ä»¥å†…"""

        return prompt

    def _convert_to_serializable(self, obj):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, dict):
            return {key: self._convert_to_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_serializable(item) for item in obj]
        elif hasattr(obj, 'strftime'):  # æ—¥æœŸæ—¶é—´å¯¹è±¡
            return str(obj)
        elif hasattr(obj, 'item'):  # numpyå¯¹è±¡
            return obj.item()
        else:
            return str(obj)

    def _convert_to_serializable_limited(self, obj, max_items=3):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼ - é™åˆ¶æ•°é‡ç‰ˆæœ¬"""
        if isinstance(obj, dict):
            limited_dict = {}
            for key, value in obj.items():
                if isinstance(value, list):
                    # é™åˆ¶åˆ—è¡¨é•¿åº¦
                    limited_dict[key] = [self._convert_to_serializable(item) for item in value[:max_items]]
                else:
                    limited_dict[key] = self._convert_to_serializable(value)
            return limited_dict
        elif isinstance(obj, list):
            return [self._convert_to_serializable(item) for item in obj[:max_items]]
        elif hasattr(obj, 'strftime'):  # æ—¥æœŸæ—¶é—´å¯¹è±¡
            return str(obj)
        elif hasattr(obj, 'item'):  # numpyå¯¹è±¡
            return obj.item()
        else:
            return str(obj)

    def _llm_intelligent_database_design(self):
        """
        ç¬¬3æ­¥: LLMæ™ºèƒ½æ•°æ®åº“è®¾è®¡
        åŸºäºæ‰€æœ‰æ–‡ä»¶çš„åˆ†æç»“æœï¼Œè®¾è®¡å®Œæ•´çš„æ•°æ®åº“æ¶æ„
        """
        logger.info("ğŸ—ï¸ LLMæ™ºèƒ½æ•°æ®åº“è®¾è®¡")

        # æ„å»ºæ•°æ®åº“è®¾è®¡æç¤ºè¯
        design_prompt = self._build_database_design_prompt()

        # è°ƒç”¨LLMè¿›è¡Œæ•°æ®åº“è®¾è®¡
        design_result = self._call_llm_unlimited_retry(design_prompt)

        if design_result:
            parsed_result = self._parse_llm_json_response(design_result)
            if parsed_result:
                self.llm_schema_design = parsed_result
                logger.info("âœ… æ•°æ®åº“è®¾è®¡å®Œæˆ")
            else:
                logger.warning("âš ï¸ æ•°æ®åº“è®¾è®¡ç»“æœè§£æå¤±è´¥")
        else:
            logger.warning("âš ï¸ æ•°æ®åº“è®¾è®¡å¤±è´¥")

    def _build_database_design_prompt(self) -> str:
        """æ„å»ºæ•°æ®åº“è®¾è®¡æç¤ºè¯"""

        # ç®€åŒ–åˆ†æç»“æœï¼ŒåªåŒ…å«å…³é”®ä¿¡æ¯
        simplified_analysis = {}
        for file_path, analysis in self.llm_file_analysis.items():
            file_name = os.path.basename(file_path)
            # åªä¿ç•™å…³é”®ä¿¡æ¯ï¼Œé¿å…JSONè¿‡å¤§
            simplified_analysis[file_name] = {
                'file_type': analysis.get('file_type', ''),
                'business_domain': analysis.get('business_domain', ''),
                'key_fields': analysis.get('key_fields', [])[:5],  # åªå–å‰5ä¸ªå­—æ®µ
                'table_suggestion': analysis.get('table_suggestion', '')
            }

        # åªå¤„ç†ä¸šåŠ¡æ•°æ®æ–‡ä»¶
        business_files = []
        for file_path, analysis in self.llm_file_analysis.items():
            if analysis.get('file_type') == 'ä¸šåŠ¡æ•°æ®æ–‡ä»¶':
                business_files.append({
                    'file_name': os.path.basename(file_path),
                    'file_path': file_path,
                    'business_domain': analysis.get('business_domain', ''),
                    'key_fields': analysis.get('key_fields', [])[:5]
                })

        prompt = f"""åŸºäºä»¥ä¸‹ä¸šåŠ¡æ•°æ®æ–‡ä»¶ï¼Œä¸ºæ¯ä¸ªæ–‡ä»¶è®¾è®¡å¯¹åº”çš„æ•°æ®åº“è¡¨ã€‚

ä¸šåŠ¡æ•°æ®æ–‡ä»¶ï¼š
{json.dumps(business_files, ensure_ascii=False, indent=2)}

è¯·ä¸ºæ¯ä¸ªä¸šåŠ¡æ•°æ®æ–‡ä»¶è®¾è®¡ä¸€ä¸ªå¯¹åº”çš„è¡¨ï¼Œè¿”å›JSONæ ¼å¼ï¼š

{{
  "tables": [
    {{
      "table_name": "åŸºäºæ–‡ä»¶åçš„è‹±æ–‡è¡¨å",
      "source_file": "å®Œæ•´çš„æºæ–‡ä»¶å",
      "description": "è¡¨çš„ä¸šåŠ¡æè¿°",
      "fields": [
        {{
          "field_name": "å­—æ®µå",
          "data_type": "TEXT",
          "is_primary_key": false,
          "description": "å­—æ®µæè¿°"
        }}
      ]
    }}
  ]
}}

è¦æ±‚ï¼š
1. å¿…é¡»ä¸ºæ¯ä¸ªä¸šåŠ¡æ•°æ®æ–‡ä»¶åˆ›å»ºä¸€ä¸ªè¡¨
2. è¡¨ååŸºäºæ–‡ä»¶åç”Ÿæˆï¼Œä½¿ç”¨è‹±æ–‡ï¼Œç¬¦åˆæ•°æ®åº“å‘½åè§„èŒƒ
3. source_fileå¿…é¡»ä¸è¾“å…¥çš„æ–‡ä»¶åå®Œå…¨ä¸€è‡´
4. å­—æ®µç±»å‹ç»Ÿä¸€ä½¿ç”¨TEXT
5. åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—
"""

        return prompt

    def _llm_guided_database_creation(self, output_db_path: str):
        """
        ç¬¬4æ­¥: LLMæŒ‡å¯¼çš„æ•°æ®åº“åˆ›å»º
        æ ¹æ®LLMè®¾è®¡çš„æ¶æ„åˆ›å»ºå®é™…çš„æ•°æ®åº“ç»“æ„
        """
        logger.info("ğŸ—„ï¸ LLMæŒ‡å¯¼çš„æ•°æ®åº“åˆ›å»º")

        if not self.llm_schema_design:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®åº“è®¾è®¡æ–¹æ¡ˆ")
            return

        try:
            # åˆ é™¤å·²å­˜åœ¨çš„æ•°æ®åº“æ–‡ä»¶
            if os.path.exists(output_db_path):
                os.remove(output_db_path)

            # åˆ›å»ºæ•°æ®åº“è¿æ¥
            conn = sqlite3.connect(output_db_path)
            cursor = conn.cursor()

            # åˆ›å»ºè¡¨ç»“æ„
            tables = self.llm_schema_design.get('tables', [])
            for table_config in tables:
                table_name = table_config['table_name']

                # ç”ŸæˆCREATE TABLEè¯­å¥
                create_sql = self._generate_create_table_sql(table_config)

                if create_sql:
                    logger.info(f"ğŸ“‹ åˆ›å»ºè¡¨: {table_name}")
                    cursor.execute(create_sql)

                    # è®°å½•åˆ›å»ºæ—¥å¿—
                    self.import_execution_log.append({
                        'step': 'create_table',
                        'table_name': table_name,
                        'sql': create_sql,
                        'timestamp': datetime.now().isoformat()
                    })

                # åˆ›å»ºç´¢å¼•
                indexes = table_config.get('indexes', [])
                for index_config in indexes:
                    if index_config['index_type'] != 'PRIMARY':  # ä¸»é”®å·²åœ¨è¡¨åˆ›å»ºæ—¶å¤„ç†
                        index_sql = self._generate_create_index_sql(table_name, index_config)
                        if index_sql:
                            logger.info(f"ğŸ” åˆ›å»ºç´¢å¼•: {index_config['index_name']}")
                            cursor.execute(index_sql)

            conn.commit()
            conn.close()

            logger.info("âœ… æ•°æ®åº“ç»“æ„åˆ›å»ºå®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆ›å»ºå¤±è´¥: {e}")
            raise

    def _generate_create_table_sql(self, table_config: Dict[str, Any]) -> str:
        """ç”ŸæˆCREATE TABLE SQLè¯­å¥"""

        table_name = table_config['table_name']
        fields = table_config.get('fields', [])

        if not fields:
            return ""

        sql_parts = [f"CREATE TABLE {table_name} ("]
        field_definitions = []
        primary_keys = []

        for field in fields:
            field_name = field['field_name']
            data_type = field['data_type']

            # æ„å»ºå­—æ®µå®šä¹‰
            field_def = f"    {field_name} {data_type}"

            # æ·»åŠ çº¦æŸ
            if not field.get('is_nullable', True):
                field_def += " NOT NULL"

            if field.get('default_value'):
                default_val = field['default_value']
                if data_type.upper().startswith(('VARCHAR', 'TEXT', 'CHAR')):
                    field_def += f" DEFAULT '{default_val}'"
                else:
                    field_def += f" DEFAULT {default_val}"

            if field.get('check_constraint'):
                field_def += f" CHECK ({field['check_constraint']})"

            field_definitions.append(field_def)

            # æ”¶é›†ä¸»é”®
            if field.get('is_primary_key', False):
                primary_keys.append(field_name)

        sql_parts.append(",\n".join(field_definitions))

        # æ·»åŠ ä¸»é”®çº¦æŸ
        if primary_keys:
            sql_parts.append(f",\n    PRIMARY KEY ({', '.join(primary_keys)})")

        sql_parts.append("\n)")

        return "".join(sql_parts)

    def _generate_create_index_sql(self, table_name: str, index_config: Dict[str, Any]) -> str:
        """ç”ŸæˆCREATE INDEX SQLè¯­å¥"""

        index_name = index_config['index_name']
        index_type = index_config['index_type']
        fields = index_config['fields']

        if index_type == 'UNIQUE':
            sql = f"CREATE UNIQUE INDEX {index_name} ON {table_name} ({', '.join(fields)})"
        else:
            sql = f"CREATE INDEX {index_name} ON {table_name} ({', '.join(fields)})"

        return sql

    def _llm_intelligent_data_import(self, output_db_path: str):
        """
        ç¬¬5æ­¥: LLMæ™ºèƒ½æ•°æ®å¯¼å…¥
        ä½¿ç”¨LLMæŒ‡å¯¼è¿›è¡Œæ™ºèƒ½çš„æ•°æ®æ˜ å°„å’Œå¯¼å…¥
        """
        logger.info("ğŸ“¥ LLMæ™ºèƒ½æ•°æ®å¯¼å…¥")

        if not self.llm_schema_design:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®åº“è®¾è®¡æ–¹æ¡ˆ")
            return

        try:
            conn = sqlite3.connect(output_db_path)

            # ä¸ºæ¯ä¸ªè¡¨å¯¼å…¥æ•°æ®
            tables = self.llm_schema_design.get('tables', [])
            for table_config in tables:
                table_name = table_config['table_name']
                source_file = table_config.get('source_file')

                if source_file:
                    # æ‰¾åˆ°å¯¹åº”çš„æºæ–‡ä»¶
                    source_file_path = self._find_source_file_path(source_file)

                    if source_file_path:
                        logger.info(f"ğŸ“Š å¯¼å…¥æ•°æ®åˆ°è¡¨: {table_name}")

                        # ä½¿ç”¨LLMæŒ‡å¯¼è¿›è¡Œæ•°æ®æ˜ å°„å’Œå¯¼å…¥
                        success = self._llm_guided_data_mapping_and_import(
                            conn, table_config, source_file_path
                        )

                        if success:
                            logger.info(f"âœ… æ•°æ®å¯¼å…¥æˆåŠŸ: {table_name}")
                        else:
                            logger.warning(f"âš ï¸ æ•°æ®å¯¼å…¥å¤±è´¥: {table_name}")
                    else:
                        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æºæ–‡ä»¶: {source_file}")

            conn.close()
            logger.info("âœ… æ•°æ®å¯¼å…¥å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
            raise

    def _find_source_file_path(self, source_file_name: str) -> Optional[str]:
        """æŸ¥æ‰¾æºæ–‡ä»¶è·¯å¾„"""
        for file_path, file_info in self.discovered_files.items():
            if file_info['file_name'] == source_file_name:
                return file_path
        return None

    def _llm_guided_data_mapping_and_import(self, conn: sqlite3.Connection,
                                          table_config: Dict[str, Any],
                                          source_file_path: str) -> bool:
        """LLMæŒ‡å¯¼çš„æ•°æ®æ˜ å°„å’Œå¯¼å…¥"""

        try:
            # è¯»å–æºæ–‡ä»¶æ•°æ®
            if source_file_path.endswith('.csv'):
                df = pd.read_csv(source_file_path)
            else:
                df = pd.read_excel(source_file_path)

            # æ„å»ºæ•°æ®æ˜ å°„æç¤ºè¯
            mapping_prompt = self._build_data_mapping_prompt(table_config, df)

            # è°ƒç”¨LLMè¿›è¡Œæ•°æ®æ˜ å°„åˆ†æ
            mapping_result = self._call_llm_unlimited_retry(mapping_prompt)

            if mapping_result:
                parsed_mapping = self._parse_llm_json_response(mapping_result)

                if parsed_mapping:
                    # æ ¹æ®LLMçš„æ˜ å°„ç»“æœè¿›è¡Œæ•°æ®è½¬æ¢
                    transformed_df = self._transform_data_with_llm_mapping(df, parsed_mapping)

                    # å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“
                    table_name = table_config['table_name']
                    transformed_df.to_sql(table_name, conn, if_exists='append', index=False)

                    # è®°å½•å¯¼å…¥æ—¥å¿—
                    self.import_execution_log.append({
                        'step': 'import_data',
                        'table_name': table_name,
                        'source_file': source_file_path,
                        'imported_rows': len(transformed_df),
                        'timestamp': datetime.now().isoformat()
                    })

                    return True

            return False

        except Exception as e:
            logger.error(f"âŒ LLMæŒ‡å¯¼çš„æ•°æ®æ˜ å°„å’Œå¯¼å…¥å¤±è´¥: {e}")
            return False

    def _build_data_mapping_prompt(self, table_config: Dict[str, Any], df: pd.DataFrame) -> str:
        """æ„å»ºæ•°æ®æ˜ å°„æç¤ºè¯"""

        # è·å–æºæ•°æ®ä¿¡æ¯
        source_columns = list(df.columns)
        sample_data = df.head(5).to_dict('records')

        # è·å–ç›®æ ‡è¡¨ç»“æ„
        target_fields = table_config.get('fields', [])

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®æ˜ å°„ä¸“å®¶ï¼Œè¯·åˆ†ææºæ•°æ®å’Œç›®æ ‡è¡¨ç»“æ„ï¼Œæä¾›ç²¾ç¡®çš„å­—æ®µæ˜ å°„å’Œæ•°æ®è½¬æ¢æ–¹æ¡ˆã€‚

## ç›®æ ‡è¡¨ç»“æ„
è¡¨å: {table_config['table_name']}
è¡¨æè¿°: {table_config.get('description', '')}

ç›®æ ‡å­—æ®µ:
{json.dumps(target_fields, ensure_ascii=False, indent=2)}

## æºæ•°æ®ä¿¡æ¯
æºå­—æ®µ: {source_columns}

æ ·æœ¬æ•°æ®:
{json.dumps(sample_data, ensure_ascii=False, indent=2)}

## æ˜ å°„ä»»åŠ¡
è¯·åˆ†ææºæ•°æ®å’Œç›®æ ‡è¡¨ç»“æ„ï¼Œæä¾›è¯¦ç»†çš„å­—æ®µæ˜ å°„æ–¹æ¡ˆã€‚ä¸è¦ä½¿ç”¨ä»»ä½•é¢„è®¾è§„åˆ™ï¼Œå®Œå…¨åŸºäºå¯¹æ•°æ®è¯­ä¹‰çš„ç†è§£è¿›è¡Œæ˜ å°„ã€‚

## åˆ†æè¦æ±‚
1. **å­—æ®µæ˜ å°„**ï¼šç¡®å®šæ¯ä¸ªç›®æ ‡å­—æ®µå¯¹åº”çš„æºå­—æ®µ
2. **æ•°æ®è½¬æ¢**ï¼šå®šä¹‰å¿…è¦çš„æ•°æ®ç±»å‹è½¬æ¢å’Œæ ¼å¼åŒ–
3. **æ•°æ®æ¸…æ´—**ï¼šè¯†åˆ«éœ€è¦æ¸…æ´—çš„æ•°æ®é—®é¢˜
4. **é»˜è®¤å€¼å¤„ç†**ï¼šä¸ºç¼ºå¤±æ•°æ®æä¾›åˆç†çš„é»˜è®¤å€¼
5. **æ•°æ®éªŒè¯**ï¼šå®šä¹‰æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥è§„åˆ™

## è¾“å‡ºæ ¼å¼
è¯·ä»¥JSONæ ¼å¼è¿”å›æ˜ å°„æ–¹æ¡ˆï¼š

{{
  "mapping_analysis": {{
    "confidence_score": 0.95,
    "mapping_strategy": "æ˜ å°„ç­–ç•¥æè¿°",
    "data_quality_assessment": "æ•°æ®è´¨é‡è¯„ä¼°"
  }},
  "field_mappings": [
    {{
      "target_field": "ç›®æ ‡å­—æ®µå",
      "source_field": "æºå­—æ®µå",
      "mapping_type": "DIRECT/TRANSFORM/CALCULATE/DEFAULT",
      "transformation_rule": "è½¬æ¢è§„åˆ™",
      "data_type_conversion": "æ•°æ®ç±»å‹è½¬æ¢",
      "default_value": "é»˜è®¤å€¼",
      "validation_rule": "éªŒè¯è§„åˆ™",
      "cleaning_operations": ["æ¸…æ´—æ“ä½œåˆ—è¡¨"],
      "confidence": 0.9
    }}
  ],
  "data_transformations": [
    {{
      "operation": "æ“ä½œç±»å‹",
      "description": "æ“ä½œæè¿°",
      "affected_fields": ["å½±å“å­—æ®µ"],
      "transformation_logic": "è½¬æ¢é€»è¾‘"
    }}
  ],
  "data_quality_issues": [
    {{
      "issue_type": "é—®é¢˜ç±»å‹",
      "affected_fields": ["å½±å“å­—æ®µ"],
      "description": "é—®é¢˜æè¿°",
      "resolution": "è§£å†³æ–¹æ¡ˆ"
    }}
  ]
}}

è¯·åŸºäºæ·±åº¦çš„è¯­ä¹‰ç†è§£è¿›è¡Œæ˜ å°„ï¼Œç¡®ä¿æ•°æ®çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚
"""

        return prompt

    def _transform_data_with_llm_mapping(self, df: pd.DataFrame,
                                       mapping_config: Dict[str, Any]) -> pd.DataFrame:
        """æ ¹æ®LLMæ˜ å°„é…ç½®è½¬æ¢æ•°æ®"""

        try:
            # åˆ›å»ºæ–°çš„DataFrameç”¨äºå­˜å‚¨è½¬æ¢åçš„æ•°æ®
            transformed_data = {}

            # æ ¹æ®å­—æ®µæ˜ å°„è¿›è¡Œæ•°æ®è½¬æ¢
            field_mappings = mapping_config.get('field_mappings', [])

            for mapping in field_mappings:
                target_field = mapping['target_field']
                source_field = mapping.get('source_field')
                mapping_type = mapping.get('mapping_type', 'DIRECT')
                default_value = mapping.get('default_value')
                transformation_rule = mapping.get('transformation_rule', '')

                if mapping_type == 'DIRECT' and source_field and source_field in df.columns:
                    # ç›´æ¥æ˜ å°„
                    transformed_data[target_field] = df[source_field]

                elif mapping_type == 'TRANSFORM' and source_field and source_field in df.columns:
                    # éœ€è¦è½¬æ¢çš„æ˜ å°„
                    transformed_data[target_field] = self._apply_transformation(
                        df[source_field], transformation_rule
                    )

                elif mapping_type == 'DEFAULT':
                    # ä½¿ç”¨é»˜è®¤å€¼
                    transformed_data[target_field] = [default_value] * len(df)

                elif mapping_type == 'CALCULATE':
                    # è®¡ç®—å­—æ®µï¼ˆåŸºäºå…¶ä»–å­—æ®µè®¡ç®—ï¼‰
                    transformed_data[target_field] = self._calculate_field(
                        df, transformation_rule
                    )

                else:
                    # å¦‚æœæ— æ³•æ˜ å°„ï¼Œä½¿ç”¨é»˜è®¤å€¼æˆ–ç©ºå€¼
                    if default_value is not None:
                        transformed_data[target_field] = [default_value] * len(df)
                    else:
                        transformed_data[target_field] = [None] * len(df)

            # åˆ›å»ºè½¬æ¢åçš„DataFrame
            result_df = pd.DataFrame(transformed_data)

            # åº”ç”¨æ•°æ®æ¸…æ´—æ“ä½œ
            result_df = self._apply_data_cleaning(result_df, mapping_config)

            return result_df

        except Exception as e:
            logger.error(f"âŒ æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            return pd.DataFrame()

    def _apply_transformation(self, series: pd.Series, transformation_rule: str) -> pd.Series:
        """åº”ç”¨æ•°æ®è½¬æ¢è§„åˆ™"""
        try:
            # è¿™é‡Œå¯ä»¥æ ¹æ®transformation_ruleå®ç°å„ç§è½¬æ¢
            # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œåªå®ç°åŸºæœ¬çš„ç±»å‹è½¬æ¢
            if 'to_string' in transformation_rule.lower():
                return series.astype(str)
            elif 'to_numeric' in transformation_rule.lower():
                return pd.to_numeric(series, errors='coerce')
            elif 'to_datetime' in transformation_rule.lower():
                return pd.to_datetime(series, errors='coerce')
            else:
                return series
        except Exception as e:
            logger.warning(f"âš ï¸ è½¬æ¢è§„åˆ™åº”ç”¨å¤±è´¥: {e}")
            return series

    def _calculate_field(self, df: pd.DataFrame, calculation_rule: str) -> pd.Series:
        """è®¡ç®—å­—æ®µå€¼"""
        try:
            # è¿™é‡Œå¯ä»¥å®ç°å¤æ‚çš„è®¡ç®—é€»è¾‘
            # ä¸ºäº†ç®€åŒ–ï¼Œè¿”å›ç©ºå€¼
            return pd.Series([None] * len(df))
        except Exception as e:
            logger.warning(f"âš ï¸ å­—æ®µè®¡ç®—å¤±è´¥: {e}")
            return pd.Series([None] * len(df))

    def _apply_data_cleaning(self, df: pd.DataFrame,
                           mapping_config: Dict[str, Any]) -> pd.DataFrame:
        """åº”ç”¨æ•°æ®æ¸…æ´—æ“ä½œ"""
        try:
            # æ ¹æ®æ˜ å°„é…ç½®ä¸­çš„æ¸…æ´—è§„åˆ™è¿›è¡Œæ•°æ®æ¸…æ´—
            data_quality_issues = mapping_config.get('data_quality_issues', [])

            for issue in data_quality_issues:
                resolution = issue.get('resolution', '')
                affected_fields = issue.get('affected_fields', [])

                for field in affected_fields:
                    if field in df.columns:
                        if 'remove_nulls' in resolution.lower():
                            df[field] = df[field].fillna('')
                        elif 'convert_type' in resolution.lower():
                            df[field] = pd.to_numeric(df[field], errors='coerce')

            return df

        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            return df

    def _llm_business_intelligence_analysis(self):
        """
        ç¬¬6æ­¥: LLMä¸šåŠ¡æ™ºèƒ½åˆ†æ
        åŸºäºå¯¼å…¥çš„æ•°æ®è¿›è¡Œæ·±åº¦çš„ä¸šåŠ¡æ™ºèƒ½åˆ†æ
        """
        logger.info("ğŸ§  LLMä¸šåŠ¡æ™ºèƒ½åˆ†æ")

        # æ„å»ºä¸šåŠ¡æ™ºèƒ½åˆ†ææç¤ºè¯
        bi_prompt = self._build_business_intelligence_prompt()

        # è°ƒç”¨LLMè¿›è¡Œä¸šåŠ¡æ™ºèƒ½åˆ†æ
        bi_result = self._call_llm_unlimited_retry(bi_prompt)

        if bi_result:
            parsed_result = self._parse_llm_json_response(bi_result)
            if parsed_result:
                self.llm_business_intelligence = parsed_result
                logger.info("âœ… ä¸šåŠ¡æ™ºèƒ½åˆ†æå®Œæˆ")
            else:
                logger.warning("âš ï¸ ä¸šåŠ¡æ™ºèƒ½åˆ†æç»“æœè§£æå¤±è´¥")
        else:
            logger.warning("âš ï¸ ä¸šåŠ¡æ™ºèƒ½åˆ†æå¤±è´¥")

    def _build_business_intelligence_prompt(self) -> str:
        """æ„å»ºä¸šåŠ¡æ™ºèƒ½åˆ†ææç¤ºè¯"""

        # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        analysis_summary = {
            'file_analysis': self.llm_file_analysis,
            'schema_design': self.llm_schema_design,
            'import_log': self.import_execution_log
        }

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ä¸šåŠ¡æ™ºèƒ½åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹æ•°æ®å¯¼å…¥å’Œåˆ†æç»“æœï¼Œæä¾›æ·±åº¦çš„ä¸šåŠ¡æ™ºèƒ½æ´å¯Ÿã€‚

## å®Œæ•´åˆ†æç»“æœ
{json.dumps(analysis_summary, ensure_ascii=False, indent=2)}

## ä¸šåŠ¡æ™ºèƒ½åˆ†æä»»åŠ¡
è¯·åŸºäºå¯¹æ•°æ®çš„æ·±åº¦ç†è§£ï¼Œæä¾›å…¨é¢çš„ä¸šåŠ¡æ™ºèƒ½åˆ†æã€‚ä¸è¦ä½¿ç”¨ä»»ä½•é¢„è®¾æ¨¡å¼ï¼Œå®Œå…¨åŸºäºæ•°æ®çš„ä¸šåŠ¡é€»è¾‘è¿›è¡Œåˆ†æã€‚

## åˆ†æç»´åº¦
1. **ä¸šåŠ¡ä»·å€¼åˆ†æ**ï¼š
   - æ•°æ®çš„æ ¸å¿ƒä¸šåŠ¡ä»·å€¼
   - å…³é”®ä¸šåŠ¡æŒ‡æ ‡è¯†åˆ«
   - ä¸šåŠ¡æµç¨‹æ´å¯Ÿ

2. **æ•°æ®å…³ç³»æ´å¯Ÿ**ï¼š
   - æ•°æ®å®ä½“é—´çš„ä¸šåŠ¡å…³ç³»
   - å…³é”®ä¸šåŠ¡è§„åˆ™å‘ç°
   - æ•°æ®é©±åŠ¨çš„ä¸šåŠ¡é€»è¾‘

3. **ä¸šåŠ¡æœ¯è¯­ä½“ç³»**ï¼š
   - æ ¸å¿ƒä¸šåŠ¡æœ¯è¯­å®šä¹‰
   - ä¸šåŠ¡æ¦‚å¿µå±‚æ¬¡ç»“æ„
   - æœ¯è¯­é—´çš„é€»è¾‘å…³ç³»

4. **æŸ¥è¯¢åœºæ™¯é¢„æµ‹**ï¼š
   - å¯èƒ½çš„ä¸šåŠ¡æŸ¥è¯¢åœºæ™¯
   - å…³é”®åˆ†æéœ€æ±‚é¢„æµ‹
   - æŠ¥è¡¨éœ€æ±‚è¯†åˆ«

5. **ä¸šåŠ¡ä¼˜åŒ–å»ºè®®**ï¼š
   - æ•°æ®è´¨é‡æ”¹è¿›å»ºè®®
   - ä¸šåŠ¡æµç¨‹ä¼˜åŒ–å»ºè®®
   - æ•°æ®æ²»ç†å»ºè®®

## è¾“å‡ºæ ¼å¼
è¯·ä»¥JSONæ ¼å¼è¿”å›ä¸šåŠ¡æ™ºèƒ½åˆ†æï¼š

{{
  "business_intelligence_summary": {{
    "business_domain": "ä¸šåŠ¡é¢†åŸŸ",
    "core_business_value": "æ ¸å¿ƒä¸šåŠ¡ä»·å€¼",
    "key_business_processes": ["å…³é”®ä¸šåŠ¡æµç¨‹"],
    "data_maturity_level": "æ•°æ®æˆç†Ÿåº¦ç­‰çº§"
  }},
  "business_entities": [
    {{
      "entity_name": "ä¸šåŠ¡å®ä½“å",
      "description": "å®ä½“æè¿°",
      "key_attributes": ["å…³é”®å±æ€§"],
      "business_rules": ["ä¸šåŠ¡è§„åˆ™"],
      "relationships": ["å…³ç³»æè¿°"]
    }}
  ],
  "business_terms_dictionary": [
    {{
      "term": "ä¸šåŠ¡æœ¯è¯­",
      "definition": "æœ¯è¯­å®šä¹‰",
      "category": "æœ¯è¯­åˆ†ç±»",
      "sql_expression": "SQLè¡¨è¾¾å¼",
      "business_context": "ä¸šåŠ¡ä¸Šä¸‹æ–‡",
      "usage_examples": ["ä½¿ç”¨ç¤ºä¾‹"],
      "related_terms": ["ç›¸å…³æœ¯è¯­"]
    }}
  ],
  "key_business_metrics": [
    {{
      "metric_name": "æŒ‡æ ‡åç§°",
      "description": "æŒ‡æ ‡æè¿°",
      "calculation_logic": "è®¡ç®—é€»è¾‘",
      "business_significance": "ä¸šåŠ¡æ„ä¹‰",
      "data_sources": ["æ•°æ®æ¥æº"],
      "update_frequency": "æ›´æ–°é¢‘ç‡"
    }}
  ],
  "query_scenarios": [
    {{
      "scenario_name": "æŸ¥è¯¢åœºæ™¯",
      "description": "åœºæ™¯æè¿°",
      "business_question": "ä¸šåŠ¡é—®é¢˜",
      "required_data": ["æ‰€éœ€æ•°æ®"],
      "expected_insights": "é¢„æœŸæ´å¯Ÿ",
      "query_complexity": "æŸ¥è¯¢å¤æ‚åº¦"
    }}
  ],
  "data_quality_insights": {{
    "overall_quality_score": 0.85,
    "quality_dimensions": [
      {{
        "dimension": "è´¨é‡ç»´åº¦",
        "score": 0.9,
        "issues": ["é—®é¢˜åˆ—è¡¨"],
        "recommendations": ["æ”¹è¿›å»ºè®®"]
      }}
    ]
  }},
  "business_optimization_recommendations": [
    {{
      "category": "ä¼˜åŒ–ç±»åˆ«",
      "recommendation": "ä¼˜åŒ–å»ºè®®",
      "business_impact": "ä¸šåŠ¡å½±å“",
      "implementation_priority": "å®æ–½ä¼˜å…ˆçº§",
      "expected_benefits": ["é¢„æœŸæ”¶ç›Š"]
    }}
  ],
  "context_configuration": {{
    "database_specific_business_terms": "ä¸šåŠ¡æœ¯è¯­é…ç½®",
    "query_scope_rules": "æŸ¥è¯¢èŒƒå›´è§„åˆ™",
    "field_mappings": "å­—æ®µæ˜ å°„é…ç½®",
    "relationship_definitions": "å…³ç³»å®šä¹‰"
  }}
}}

è¯·åŸºäºæ·±åº¦çš„ä¸šåŠ¡ç†è§£æä¾›æ´å¯Ÿï¼Œç¡®ä¿åˆ†æç»“æœå…·æœ‰å®é™…çš„ä¸šåŠ¡æŒ‡å¯¼ä»·å€¼ã€‚
"""

        return prompt

    def _llm_generate_comprehensive_report(self, output_db_path: str,
                                         start_time: datetime) -> Dict[str, Any]:
        """
        ç¬¬7æ­¥: LLMç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        ç”Ÿæˆå®Œæ•´çš„å¤„ç†æŠ¥å‘Šå’Œä¸šåŠ¡æ´å¯Ÿ
        """
        logger.info("ğŸ“‹ LLMç”Ÿæˆç»¼åˆæŠ¥å‘Š")

        execution_time = (datetime.now() - start_time).total_seconds()

        # ç»Ÿè®¡å¯¼å…¥æ•°æ®
        total_imported_rows = sum(
            log.get('imported_rows', 0) for log in self.import_execution_log
            if log.get('step') == 'import_data'
        )

        total_tables_created = len([
            log for log in self.import_execution_log
            if log.get('step') == 'create_table'
        ])

        # æ„å»ºæŠ¥å‘Šç”Ÿæˆæç¤ºè¯
        report_prompt = self._build_report_generation_prompt(
            output_db_path, execution_time, total_imported_rows, total_tables_created
        )

        # è°ƒç”¨LLMç”ŸæˆæŠ¥å‘Š
        report_result = self._call_llm_unlimited_retry(report_prompt)

        if report_result:
            parsed_report = self._parse_llm_json_response(report_result)

            if parsed_report:
                # åˆå¹¶åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
                final_report = {
                    'success': True,
                    'execution_time': execution_time,
                    'output_database': output_db_path,
                    'processing_summary': {
                        'total_files_processed': len(self.discovered_files),
                        'total_tables_created': total_tables_created,
                        'total_imported_rows': total_imported_rows,
                        'llm_analysis_success_rate': len(self.llm_file_analysis) / len(self.discovered_files) if self.discovered_files else 0
                    },
                    'llm_comprehensive_analysis': parsed_report,
                    'business_intelligence': self.llm_business_intelligence,
                    'detailed_execution_log': self.import_execution_log
                }

                logger.info("âœ… ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
                return final_report

        # å¦‚æœLLMæŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œè¿”å›åŸºç¡€æŠ¥å‘Š
        logger.warning("âš ï¸ LLMæŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œè¿”å›åŸºç¡€æŠ¥å‘Š")
        return {
            'success': True,
            'execution_time': execution_time,
            'output_database': output_db_path,
            'processing_summary': {
                'total_files_processed': len(self.discovered_files),
                'total_tables_created': total_tables_created,
                'total_imported_rows': total_imported_rows,
                'llm_analysis_success_rate': len(self.llm_file_analysis) / len(self.discovered_files) if self.discovered_files else 0
            },
            'business_intelligence': self.llm_business_intelligence,
            'detailed_execution_log': self.import_execution_log
        }

    def _build_report_generation_prompt(self, output_db_path: str, execution_time: float,
                                       total_imported_rows: int, total_tables_created: int) -> str:
        """æ„å»ºæŠ¥å‘Šç”Ÿæˆæç¤ºè¯"""

        # æ”¶é›†æ‰€æœ‰å¤„ç†ç»“æœ
        complete_analysis = {
            'file_analysis': self.llm_file_analysis,
            'schema_design': self.llm_schema_design,
            'business_intelligence': self.llm_business_intelligence,
            'execution_log': self.import_execution_log,
            'statistics': {
                'execution_time': execution_time,
                'total_imported_rows': total_imported_rows,
                'total_tables_created': total_tables_created,
                'total_files_processed': len(self.discovered_files)
            }
        }

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®é¡¹ç›®æ€»ç»“ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹å®Œæ•´çš„æ•°æ®å¯¼å…¥å’Œåˆ†æç»“æœï¼Œç”Ÿæˆä¸€ä»½å…¨é¢çš„é¡¹ç›®æŠ¥å‘Šã€‚

## å®Œæ•´å¤„ç†ç»“æœ
{json.dumps(complete_analysis, ensure_ascii=False, indent=2)}

## æŠ¥å‘Šç”Ÿæˆä»»åŠ¡
è¯·åŸºäºæ•´ä¸ªæ•°æ®å¯¼å…¥å’Œåˆ†æè¿‡ç¨‹ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„é¡¹ç›®æ€»ç»“æŠ¥å‘Šã€‚ä¸è¦ä½¿ç”¨ä»»ä½•æ¨¡æ¿ï¼Œå®Œå…¨åŸºäºå®é™…çš„å¤„ç†ç»“æœè¿›è¡Œæ€»ç»“ã€‚

## æŠ¥å‘Šè¦æ±‚
1. **æ‰§è¡Œæ€»ç»“**ï¼šæ•´ä¸ªè¿‡ç¨‹çš„é«˜å±‚æ¬¡æ€»ç»“
2. **æŠ€æœ¯æˆæœ**ï¼šæŠ€æœ¯å®ç°çš„å…³é”®æˆæœ
3. **ä¸šåŠ¡ä»·å€¼**ï¼šä¸ºä¸šåŠ¡å¸¦æ¥çš„ä»·å€¼å’Œæ´å¯Ÿ
4. **è´¨é‡è¯„ä¼°**ï¼šæ•°æ®è´¨é‡å’Œå¤„ç†è´¨é‡è¯„ä¼°
5. **åç»­å»ºè®®**ï¼šåŸºäºåˆ†æç»“æœçš„åç»­è¡ŒåŠ¨å»ºè®®

## è¾“å‡ºæ ¼å¼
è¯·ä»¥JSONæ ¼å¼è¿”å›å®Œæ•´æŠ¥å‘Šï¼š

{{
  "executive_summary": {{
    "project_overview": "é¡¹ç›®æ¦‚è¿°",
    "key_achievements": ["å…³é”®æˆå°±"],
    "business_impact": "ä¸šåŠ¡å½±å“",
    "technical_highlights": ["æŠ€æœ¯äº®ç‚¹"]
  }},
  "technical_results": {{
    "data_processing_summary": "æ•°æ®å¤„ç†æ€»ç»“",
    "database_design_quality": "æ•°æ®åº“è®¾è®¡è´¨é‡è¯„ä¼°",
    "automation_level": "è‡ªåŠ¨åŒ–ç¨‹åº¦",
    "technical_innovations": ["æŠ€æœ¯åˆ›æ–°ç‚¹"]
  }},
  "business_insights": {{
    "domain_understanding": "ä¸šåŠ¡é¢†åŸŸç†è§£",
    "key_business_discoveries": ["å…³é”®ä¸šåŠ¡å‘ç°"],
    "value_propositions": ["ä»·å€¼ä¸»å¼ "],
    "business_readiness": "ä¸šåŠ¡å°±ç»ªåº¦"
  }},
  "data_quality_assessment": {{
    "overall_quality_rating": "æ•´ä½“è´¨é‡è¯„çº§",
    "quality_strengths": ["è´¨é‡ä¼˜åŠ¿"],
    "quality_concerns": ["è´¨é‡å…³æ³¨ç‚¹"],
    "data_governance_recommendations": ["æ•°æ®æ²»ç†å»ºè®®"]
  }},
  "implementation_success": {{
    "success_metrics": [
      {{
        "metric": "æˆåŠŸæŒ‡æ ‡",
        "value": "æŒ‡æ ‡å€¼",
        "assessment": "è¯„ä¼°ç»“æœ"
      }}
    ],
    "challenges_overcome": ["å…‹æœçš„æŒ‘æˆ˜"],
    "lessons_learned": ["ç»éªŒæ•™è®­"]
  }},
  "future_recommendations": {{
    "immediate_actions": ["å³æ—¶è¡ŒåŠ¨"],
    "short_term_goals": ["çŸ­æœŸç›®æ ‡"],
    "long_term_vision": ["é•¿æœŸæ„¿æ™¯"],
    "investment_priorities": ["æŠ•èµ„ä¼˜å…ˆçº§"]
  }},
  "stakeholder_communications": {{
    "executive_message": "ç»™é«˜ç®¡çš„ä¿¡æ¯",
    "technical_team_message": "ç»™æŠ€æœ¯å›¢é˜Ÿçš„ä¿¡æ¯",
    "business_user_message": "ç»™ä¸šåŠ¡ç”¨æˆ·çš„ä¿¡æ¯"
  }}
}}

è¯·åŸºäºå®é™…çš„å¤„ç†ç»“æœå’Œåˆ†ææ´å¯Ÿç”ŸæˆæŠ¥å‘Šï¼Œç¡®ä¿æŠ¥å‘Šå…·æœ‰å®é™…çš„æŒ‡å¯¼ä»·å€¼å’Œå¯æ“ä½œæ€§ã€‚
"""

        return prompt


# å‘åå…¼å®¹çš„åˆ«åå’Œä¾¿æ·å‡½æ•°
PureLLMIntelligentDataImporter = IntelligentDataImporter


def create_intelligent_importer(api_key: Optional[str] = None) -> IntelligentDataImporter:
    """åˆ›å»ºæ™ºèƒ½æ•°æ®å¯¼å…¥å™¨å®ä¾‹"""
    return IntelligentDataImporter(api_key)


def quick_intelligent_import(file_paths: List[str], output_db_path: str,
                           api_key: Optional[str] = None) -> Dict[str, Any]:
    """å¿«é€Ÿæ™ºèƒ½å¯¼å…¥å‡½æ•°"""
    importer = create_intelligent_importer(api_key)
    return importer.process_batch_import(file_paths, output_db_path)