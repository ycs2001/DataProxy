#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DataProxy LLMæ™ºèƒ½æ•°æ®å¯¼å…¥å¼•æ“ v2.0
åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„çœŸæ­£æ™ºèƒ½åŒ–æ•°æ®åˆ†æå’Œå¯¼å…¥ç³»ç»Ÿ

æ ¸å¿ƒç‰¹æ€§:
1. LLMé©±åŠ¨çš„æ™ºèƒ½æ–‡ä»¶åˆ†æ
2. è‡ªç„¶è¯­è¨€ç†è§£çš„å­—æ®µæ˜ å°„
3. æ™ºèƒ½ä¸šåŠ¡æœ¯è¯­æå–
4. è‡ªåŠ¨åŒ–æ•°æ®åº“è®¾è®¡
5. å®Œæ•´çš„ä¸Šä¸‹æ–‡é…ç½®ç”Ÿæˆ
"""

import os
import sqlite3
import pandas as pd
import json
import re
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging
import time

# LLM API ç›¸å…³å¯¼å…¥
from langchain_openai import ChatOpenAI
from langchain.schema.messages import HumanMessage, SystemMessage

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class LLMIntelligentDataImporter:
    """
    åŸºäºLLMçš„æ™ºèƒ½æ•°æ®å¯¼å…¥å¼•æ“ v2.0
    
    ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡ŒçœŸæ­£çš„æ™ºèƒ½åŒ–æ•°æ®åˆ†æå’Œå¯¼å…¥
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½æ•°æ®å¯¼å…¥å¼•æ“
        
        Args:
            api_key: LLM APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è·å–
        """
        # LLMå®¢æˆ·ç«¯åˆå§‹åŒ–
        self.llm_client = self._init_llm_client(api_key)
        
        # æ•°æ®å­˜å‚¨
        self.source_files = {}  # æ•°æ®æºæ–‡ä»¶ä¿¡æ¯
        self.dictionary_files = {}  # æ•°æ®å­—å…¸æ–‡ä»¶ä¿¡æ¯
        self.llm_analysis_results = {}  # LLMåˆ†æç»“æœ
        self.database_schema = {}  # æ•°æ®åº“è®¾è®¡æ–¹æ¡ˆ
        self.import_log = []  # è¯¦ç»†çš„å¯¼å…¥æ—¥å¿—
        
        # é…ç½®å‚æ•°
        self.max_sample_rows = 10  # å‘é€ç»™LLMçš„æ ·æœ¬æ•°æ®è¡Œæ•°
        self.max_retry_attempts = 3  # LLM APIè°ƒç”¨é‡è¯•æ¬¡æ•°
        
        logger.info("ğŸš€ LLMæ™ºèƒ½æ•°æ®å¯¼å…¥å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _init_llm_client(self, api_key: Optional[str] = None) -> Optional[ChatOpenAI]:
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            if not api_key:
                api_key = os.getenv('DEEPSEEK_API_KEY')
            
            if not api_key:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°LLM APIå¯†é’¥ï¼Œå°†ä½¿ç”¨åŸºç¡€è§„åˆ™æ¨¡å¼")
                return None
            
            client = ChatOpenAI(
                model="deepseek-chat",
                openai_api_key=api_key,
                openai_api_base="https://api.deepseek.com",
                temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿åˆ†æçš„ä¸€è‡´æ€§
                max_tokens=4000,  # è¶³å¤Ÿçš„tokenç”¨äºè¯¦ç»†åˆ†æ
                timeout=60  # 60ç§’è¶…æ—¶
            )
            
            logger.info("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return client
            
        except Exception as e:
            logger.error(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    def process_batch_import(self, data_source_dir: str, data_dict_dir: str, output_db_path: str) -> Dict[str, Any]:
        """
        LLMé©±åŠ¨çš„æ™ºèƒ½æ‰¹é‡å¯¼å…¥ä¸»æµç¨‹
        
        Args:
            data_source_dir: æ•°æ®æºç›®å½•è·¯å¾„
            data_dict_dir: æ•°æ®å­—å…¸ç›®å½•è·¯å¾„
            output_db_path: è¾“å‡ºæ•°æ®åº“è·¯å¾„
            
        Returns:
            è¯¦ç»†çš„å¤„ç†æŠ¥å‘Š
        """
        logger.info("ğŸš€ å¼€å§‹LLMæ™ºèƒ½æ•°æ®å¯¼å…¥å¤„ç†")
        start_time = time.time()
        
        try:
            # ç¬¬1æ­¥: æ–‡ä»¶å‘ç°å’Œé¢„å¤„ç†
            self._discover_and_preprocess_files(data_source_dir, data_dict_dir)
            
            # ç¬¬2æ­¥: LLMæ™ºèƒ½åˆ†æ
            self._perform_llm_analysis()
            
            # ç¬¬3æ­¥: æ•°æ®åº“è®¾è®¡ç”Ÿæˆ
            self._generate_database_design()
            
            # ç¬¬4æ­¥: åˆ›å»ºæ•°æ®åº“ç»“æ„
            self._create_database_structure(output_db_path)
            
            # ç¬¬5æ­¥: æ‰§è¡Œæ•°æ®å¯¼å…¥
            self._import_data_with_llm_guidance(output_db_path)
            
            # ç¬¬6æ­¥: ç”Ÿæˆä¸Šä¸‹æ–‡é…ç½®
            self._generate_context_configuration(output_db_path)
            
            # ç¬¬7æ­¥: ç”Ÿæˆå¤„ç†æŠ¥å‘Š
            report = self._generate_comprehensive_report(output_db_path, start_time)
            
            logger.info("âœ… LLMæ™ºèƒ½æ•°æ®å¯¼å…¥å®Œæˆ")
            return report
            
        except Exception as e:
            logger.error(f"âŒ LLMæ™ºèƒ½æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'execution_time': time.time() - start_time
            }
    
    def _discover_and_preprocess_files(self, data_source_dir: str, data_dict_dir: str):
        """
        ç¬¬1æ­¥: æ–‡ä»¶å‘ç°å’Œé¢„å¤„ç†
        """
        logger.info("ğŸ“‚ å¼€å§‹æ–‡ä»¶å‘ç°å’Œé¢„å¤„ç†")
        
        # å‘ç°æ•°æ®æºæ–‡ä»¶
        self._discover_source_files(data_source_dir)
        
        # å‘ç°æ•°æ®å­—å…¸æ–‡ä»¶
        self._discover_dictionary_files(data_dict_dir)
        
        # é¢„å¤„ç†æ–‡ä»¶å†…å®¹
        self._preprocess_file_contents()
        
        logger.info(f"ğŸ“‚ æ–‡ä»¶å‘ç°å®Œæˆ: {len(self.source_files)} ä¸ªæ•°æ®æº, {len(self.dictionary_files)} ä¸ªæ•°æ®å­—å…¸")
    
    def _discover_source_files(self, data_source_dir: str):
        """å‘ç°æ•°æ®æºæ–‡ä»¶"""
        if not os.path.exists(data_source_dir):
            logger.warning(f"âš ï¸ æ•°æ®æºç›®å½•ä¸å­˜åœ¨: {data_source_dir}")
            return
        
        for file_name in os.listdir(data_source_dir):
            if file_name.endswith(('.xlsx', '.xls', '.csv')):
                file_path = os.path.join(data_source_dir, file_name)
                
                try:
                    # è¯»å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
                    if file_name.endswith('.csv'):
                        df = pd.read_csv(file_path, nrows=self.max_sample_rows)
                    else:
                        df = pd.read_excel(file_path, nrows=self.max_sample_rows)
                    
                    self.source_files[file_name] = {
                        'file_path': file_path,
                        'file_name': file_name,
                        'columns': list(df.columns),
                        'sample_data': df.to_dict('records'),
                        'total_rows': len(df),
                        'data_types': df.dtypes.to_dict()
                    }
                    
                    logger.info(f"ğŸ“Š å‘ç°æ•°æ®æºæ–‡ä»¶: {file_name} ({len(df.columns)} åˆ—)")
                    
                except Exception as e:
                    logger.error(f"âŒ è¯»å–æ•°æ®æºæ–‡ä»¶å¤±è´¥ {file_name}: {e}")
    
    def _discover_dictionary_files(self, data_dict_dir: str):
        """å‘ç°æ•°æ®å­—å…¸æ–‡ä»¶"""
        if not os.path.exists(data_dict_dir):
            logger.warning(f"âš ï¸ æ•°æ®å­—å…¸ç›®å½•ä¸å­˜åœ¨: {data_dict_dir}")
            return
        
        for file_name in os.listdir(data_dict_dir):
            if file_name.endswith(('.xlsx', '.xls', '.csv')) and 'æ•°æ®å­—å…¸' in file_name:
                file_path = os.path.join(data_dict_dir, file_name)
                
                try:
                    # è¯»å–æ•°æ®å­—å…¸æ–‡ä»¶
                    if file_name.endswith('.csv'):
                        df = pd.read_csv(file_path)
                    else:
                        df = pd.read_excel(file_path)
                    
                    self.dictionary_files[file_name] = {
                        'file_path': file_path,
                        'file_name': file_name,
                        'columns': list(df.columns),
                        'content': df.to_dict('records'),
                        'total_rows': len(df)
                    }
                    
                    logger.info(f"ğŸ“š å‘ç°æ•°æ®å­—å…¸æ–‡ä»¶: {file_name} ({len(df)} è¡Œå®šä¹‰)")
                    
                except Exception as e:
                    logger.error(f"âŒ è¯»å–æ•°æ®å­—å…¸æ–‡ä»¶å¤±è´¥ {file_name}: {e}")
    
    def _preprocess_file_contents(self):
        """é¢„å¤„ç†æ–‡ä»¶å†…å®¹ï¼Œä¸ºLLMåˆ†æåšå‡†å¤‡"""
        logger.info("ğŸ”„ é¢„å¤„ç†æ–‡ä»¶å†…å®¹")
        
        # ä¸ºæ¯ä¸ªæ•°æ®æºæ–‡ä»¶å‡†å¤‡åˆ†ææ•°æ®
        for file_name, file_info in self.source_files.items():
            # æ•°æ®ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            file_info['data_types_str'] = {k: str(v) for k, v in file_info['data_types'].items()}
            
            # ç”Ÿæˆå­—æ®µç»Ÿè®¡ä¿¡æ¯
            file_info['field_stats'] = self._generate_field_statistics(file_info)
    
    def _generate_field_statistics(self, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå­—æ®µç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        try:
            df = pd.DataFrame(file_info['sample_data'])
            
            for column in df.columns:
                col_stats = {
                    'data_type': str(df[column].dtype),
                    'null_count': int(df[column].isnull().sum()),
                    'unique_count': int(df[column].nunique()),
                    'sample_values': df[column].dropna().head(3).tolist()
                }
                
                # æ•°å€¼ç±»å‹çš„é¢å¤–ç»Ÿè®¡
                if pd.api.types.is_numeric_dtype(df[column]):
                    col_stats.update({
                        'min_value': float(df[column].min()) if not df[column].empty else None,
                        'max_value': float(df[column].max()) if not df[column].empty else None,
                        'mean_value': float(df[column].mean()) if not df[column].empty else None
                    })
                
                stats[column] = col_stats
                
        except Exception as e:
            logger.warning(f"âš ï¸ ç”Ÿæˆå­—æ®µç»Ÿè®¡å¤±è´¥: {e}")
        
        return stats

    def _perform_llm_analysis(self):
        """
        ç¬¬2æ­¥: LLMæ™ºèƒ½åˆ†æ
        """
        logger.info("ğŸ§  å¼€å§‹LLMæ™ºèƒ½åˆ†æ")

        if not self.llm_client:
            logger.warning("âš ï¸ LLMå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡æ™ºèƒ½åˆ†æ")
            return

        # åˆ†ææ¯ä¸ªæ•°æ®æºæ–‡ä»¶
        for file_name, file_info in self.source_files.items():
            logger.info(f"ğŸ” åˆ†ææ•°æ®æºæ–‡ä»¶: {file_name}")

            # æŸ¥æ‰¾å¯¹åº”çš„æ•°æ®å­—å…¸
            dict_info = self._find_matching_dictionary(file_name)

            # æ‰§è¡ŒLLMåˆ†æ
            analysis_result = self._analyze_file_with_llm(file_info, dict_info)

            if analysis_result:
                self.llm_analysis_results[file_name] = analysis_result
                logger.info(f"âœ… å®Œæˆåˆ†æ: {file_name}")
            else:
                logger.warning(f"âš ï¸ åˆ†æå¤±è´¥: {file_name}")

    def _find_matching_dictionary(self, source_file_name: str) -> Optional[Dict[str, Any]]:
        """æŸ¥æ‰¾åŒ¹é…çš„æ•°æ®å­—å…¸æ–‡ä»¶"""

        # æå–æºæ–‡ä»¶çš„å…³é”®è¯
        source_keywords = self._extract_keywords_from_filename(source_file_name)

        best_match = None
        best_score = 0

        for dict_name, dict_info in self.dictionary_files.items():
            dict_keywords = self._extract_keywords_from_filename(dict_name)

            # è®¡ç®—åŒ¹é…åˆ†æ•°
            score = len(set(source_keywords) & set(dict_keywords))

            if score > best_score:
                best_score = score
                best_match = dict_info

        if best_match:
            logger.info(f"ğŸ“š æ‰¾åˆ°åŒ¹é…çš„æ•°æ®å­—å…¸: {best_match['file_name']}")

        return best_match

    def _extract_keywords_from_filename(self, filename: str) -> List[str]:
        """ä»æ–‡ä»¶åæå–å…³é”®è¯"""
        # ç§»é™¤æ‰©å±•åå’Œå¸¸è§å‰ç¼€
        name = os.path.splitext(filename)[0]
        name = re.sub(r'^æ•°æ®å­—å…¸[-_]?', '', name)

        # æå–ä¸­è‹±æ–‡å…³é”®è¯
        keywords = []

        # ä¸­æ–‡å…³é”®è¯
        chinese_keywords = re.findall(r'[\u4e00-\u9fff]+', name)
        keywords.extend(chinese_keywords)

        # è‹±æ–‡å…³é”®è¯
        english_keywords = re.findall(r'[A-Z_]+', name)
        keywords.extend(english_keywords)

        return keywords

    def _analyze_file_with_llm(self, file_info: Dict[str, Any], dict_info: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨LLMåˆ†ææ–‡ä»¶"""

        try:
            # æ„å»ºåˆ†ææç¤ºè¯
            analysis_prompt = self._build_analysis_prompt(file_info, dict_info)

            # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            response = self._call_llm_with_retry(analysis_prompt)

            if response:
                # è§£æLLMå“åº”
                analysis_result = self._parse_llm_analysis_response(response)
                return analysis_result

        except Exception as e:
            logger.error(f"âŒ LLMåˆ†æå¤±è´¥: {e}")

        return None

    def _build_analysis_prompt(self, file_info: Dict[str, Any], dict_info: Optional[Dict[str, Any]]) -> str:
        """æ„å»ºLLMåˆ†ææç¤ºè¯"""

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åº“è®¾è®¡ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹æ•°æ®æ–‡ä»¶å¹¶æä¾›è¯¦ç»†çš„åˆ†æç»“æœã€‚

## æ•°æ®æºæ–‡ä»¶ä¿¡æ¯
æ–‡ä»¶å: {file_info['file_name']}
å­—æ®µåˆ—è¡¨: {', '.join(file_info['columns'])}
æ•°æ®ç±»å‹: {json.dumps(file_info['data_types_str'], ensure_ascii=False, indent=2)}
æ ·æœ¬æ•°æ®: {json.dumps(file_info['sample_data'][:3], ensure_ascii=False, indent=2)}
å­—æ®µç»Ÿè®¡: {json.dumps(file_info['field_stats'], ensure_ascii=False, indent=2)}
"""

        if dict_info:
            prompt += f"""
## å¯¹åº”æ•°æ®å­—å…¸ä¿¡æ¯
æ•°æ®å­—å…¸æ–‡ä»¶: {dict_info['file_name']}
å­—å…¸åˆ—å: {', '.join(dict_info['columns'])}
å­—å…¸å†…å®¹: {json.dumps(dict_info['content'][:5], ensure_ascii=False, indent=2)}
"""

        prompt += """
## è¯·æä¾›ä»¥ä¸‹åˆ†æç»“æœï¼ˆä»¥JSONæ ¼å¼è¿”å›ï¼‰:

{
  "table_name": "æ¨èçš„æ ‡å‡†è¡¨åï¼ˆè‹±æ–‡å¤§å†™ï¼Œä¸‹åˆ’çº¿åˆ†éš”ï¼‰",
  "table_description": "è¡¨çš„ä¸šåŠ¡å«ä¹‰æè¿°",
  "fields": [
    {
      "original_name": "åŸå§‹å­—æ®µå",
      "standard_name": "æ ‡å‡†åŒ–å­—æ®µåï¼ˆè‹±æ–‡ï¼‰",
      "chinese_name": "ä¸­æ–‡åç§°",
      "data_type": "æ¨èçš„æ•°æ®ç±»å‹ï¼ˆå¦‚VARCHAR(50), INTEGER, DECIMAL(18,2)ç­‰ï¼‰",
      "is_primary_key": true/false,
      "is_required": true/false,
      "business_meaning": "å­—æ®µçš„ä¸šåŠ¡å«ä¹‰",
      "sample_values": ["æ ·æœ¬å€¼1", "æ ·æœ¬å€¼2"],
      "constraints": "çº¦æŸæ¡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰"
    }
  ],
  "business_terms": [
    {
      "term_name": "ä¸šåŠ¡æœ¯è¯­åç§°",
      "definition": "æœ¯è¯­å®šä¹‰",
      "sql_condition": "å¯¹åº”çš„SQLæ¡ä»¶",
      "applicable_fields": ["é€‚ç”¨å­—æ®µåˆ—è¡¨"]
    }
  ],
  "relationships": [
    {
      "type": "å¤–é”®å…³ç³»ç±»å‹",
      "field": "å…³è”å­—æ®µ",
      "reference_table": "å¼•ç”¨è¡¨å",
      "reference_field": "å¼•ç”¨å­—æ®µ",
      "description": "å…³ç³»æè¿°"
    }
  ],
  "data_quality_issues": [
    "å‘ç°çš„æ•°æ®è´¨é‡é—®é¢˜"
  ],
  "recommendations": [
    "æ•°æ®å¤„ç†å»ºè®®"
  ]
}

è¯·åŸºäºé“¶è¡Œä¸šåŠ¡é¢†åŸŸçŸ¥è¯†è¿›è¡Œåˆ†æï¼Œç‰¹åˆ«å…³æ³¨ï¼š
1. å®¢æˆ·ä¿¡æ¯ã€è´·æ¬¾åˆåŒã€é£é™©åˆ†ç±»ç­‰é“¶è¡Œæ ¸å¿ƒä¸šåŠ¡
2. å¯¹å…¬æœ‰æ•ˆæˆ·ï¼ˆå­˜æ¬¾ä½™é¢â‰¥10ä¸‡ï¼‰ã€ä¸è‰¯è´·æ¬¾ç­‰é‡è¦ä¸šåŠ¡æ¦‚å¿µ
3. å­—æ®µçš„ä¸šåŠ¡å«ä¹‰å’Œæ•°æ®è´¨é‡
4. è¡¨é—´çš„é€»è¾‘å…³ç³»

åªè¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""

        return prompt

    def _call_llm_with_retry(self, prompt: str) -> Optional[str]:
        """å¸¦é‡è¯•æœºåˆ¶çš„LLMè°ƒç”¨"""

        for attempt in range(self.max_retry_attempts):
            try:
                logger.info(f"ğŸ¤– è°ƒç”¨LLMåˆ†æ (å°è¯• {attempt + 1}/{self.max_retry_attempts})")

                messages = [
                    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åº“è®¾è®¡å’Œæ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿é“¶è¡Œä¸šåŠ¡é¢†åŸŸçš„æ•°æ®å»ºæ¨¡ã€‚"),
                    HumanMessage(content=prompt)
                ]

                response = self.llm_client.invoke(messages)

                if response and response.content:
                    logger.info("âœ… LLMåˆ†æå®Œæˆ")
                    return response.content.strip()

            except Exception as e:
                logger.warning(f"âš ï¸ LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < self.max_retry_attempts - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

        logger.error("âŒ LLMè°ƒç”¨æœ€ç»ˆå¤±è´¥")
        return None

    def _parse_llm_analysis_response(self, response: str) -> Optional[Dict[str, Any]]:
        """è§£æLLMåˆ†æå“åº”"""

        try:
            # å°è¯•æå–JSONå†…å®¹
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                analysis_result = json.loads(json_str)

                # éªŒè¯å¿…è¦å­—æ®µ
                required_fields = ['table_name', 'fields']
                if all(field in analysis_result for field in required_fields):
                    logger.info("âœ… LLMåˆ†æç»“æœè§£ææˆåŠŸ")
                    return analysis_result
                else:
                    logger.warning("âš ï¸ LLMåˆ†æç»“æœç¼ºå°‘å¿…è¦å­—æ®µ")

        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"âŒ å“åº”è§£æå¤±è´¥: {e}")

        return None

    def _generate_database_design(self):
        """
        ç¬¬3æ­¥: æ•°æ®åº“è®¾è®¡ç”Ÿæˆ
        """
        logger.info("ğŸ—ï¸ å¼€å§‹ç”Ÿæˆæ•°æ®åº“è®¾è®¡")

        # åŸºäºLLMåˆ†æç»“æœç”Ÿæˆæ•°æ®åº“è®¾è®¡
        for file_name, analysis in self.llm_analysis_results.items():
            table_name = analysis.get('table_name')
            if table_name:
                self.database_schema[table_name] = {
                    'source_file': file_name,
                    'table_name': table_name,
                    'description': analysis.get('table_description', ''),
                    'fields': analysis.get('fields', []),
                    'business_terms': analysis.get('business_terms', []),
                    'relationships': analysis.get('relationships', []),
                    'create_sql': self._generate_create_table_sql(analysis)
                }

                logger.info(f"âœ… ç”Ÿæˆè¡¨è®¾è®¡: {table_name}")

        logger.info(f"ğŸ—ï¸ æ•°æ®åº“è®¾è®¡å®Œæˆï¼Œå…± {len(self.database_schema)} ä¸ªè¡¨")

    def _generate_create_table_sql(self, analysis: Dict[str, Any]) -> str:
        """ç”ŸæˆCREATE TABLE SQLè¯­å¥"""

        table_name = analysis['table_name']
        fields = analysis.get('fields', [])

        if not fields:
            return ""

        sql_parts = [f"CREATE TABLE {table_name} ("]
        field_definitions = []
        primary_keys = []

        for field in fields:
            field_name = field.get('standard_name', field.get('original_name', ''))
            data_type = field.get('data_type', 'VARCHAR(255)')
            is_required = field.get('is_required', False)
            is_primary_key = field.get('is_primary_key', False)

            # æ„å»ºå­—æ®µå®šä¹‰
            field_def = f"    {field_name} {data_type}"

            if is_required:
                field_def += " NOT NULL"

            field_definitions.append(field_def)

            if is_primary_key:
                primary_keys.append(field_name)

        sql_parts.append(",\n".join(field_definitions))

        # æ·»åŠ ä¸»é”®çº¦æŸ
        if primary_keys:
            sql_parts.append(f",\n    PRIMARY KEY ({', '.join(primary_keys)})")

        sql_parts.append("\n)")

        return "".join(sql_parts)

    def _create_database_structure(self, output_db_path: str):
        """
        ç¬¬4æ­¥: åˆ›å»ºæ•°æ®åº“ç»“æ„
        """
        logger.info("ğŸ—„ï¸ å¼€å§‹åˆ›å»ºæ•°æ®åº“ç»“æ„")

        try:
            # å¦‚æœæ•°æ®åº“æ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ é™¤å®ƒ
            if os.path.exists(output_db_path):
                os.remove(output_db_path)

            conn = sqlite3.connect(output_db_path)
            cursor = conn.cursor()

            # åˆ›å»ºæ‰€æœ‰è¡¨
            for table_name, schema in self.database_schema.items():
                create_sql = schema['create_sql']
                if create_sql:
                    logger.info(f"ğŸ“‹ åˆ›å»ºè¡¨: {table_name}")
                    cursor.execute(create_sql)

                    # è®°å½•åˆ›å»ºæ—¥å¿—
                    self.import_log.append({
                        'step': 'create_table',
                        'table_name': table_name,
                        'sql': create_sql,
                        'timestamp': datetime.now().isoformat()
                    })

            conn.commit()
            conn.close()

            logger.info("âœ… æ•°æ®åº“ç»“æ„åˆ›å»ºå®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ç»“æ„åˆ›å»ºå¤±è´¥: {e}")
            raise

    def _import_data_with_llm_guidance(self, output_db_path: str):
        """
        ç¬¬5æ­¥: æ‰§è¡Œæ•°æ®å¯¼å…¥
        """
        logger.info("ğŸ“¥ å¼€å§‹æ•°æ®å¯¼å…¥")

        try:
            conn = sqlite3.connect(output_db_path)

            for table_name, schema in self.database_schema.items():
                source_file = schema['source_file']
                file_info = self.source_files[source_file]

                logger.info(f"ğŸ“Š å¯¼å…¥æ•°æ®åˆ°è¡¨: {table_name}")

                # è¯»å–å®Œæ•´æ•°æ®æ–‡ä»¶
                if source_file.endswith('.csv'):
                    df = pd.read_csv(file_info['file_path'])
                else:
                    df = pd.read_excel(file_info['file_path'])

                # æ ¹æ®LLMåˆ†æç»“æœè¿›è¡Œå­—æ®µæ˜ å°„
                df_mapped = self._map_fields_with_llm_guidance(df, schema)

                # æ•°æ®æ¸…æ´—
                df_cleaned = self._clean_data_with_llm_guidance(df_mapped, schema)

                # å¯¼å…¥æ•°æ®
                df_cleaned.to_sql(table_name, conn, if_exists='append', index=False)

                logger.info(f"âœ… æˆåŠŸå¯¼å…¥ {len(df_cleaned)} è¡Œæ•°æ®åˆ° {table_name}")

                # è®°å½•å¯¼å…¥æ—¥å¿—
                self.import_log.append({
                    'step': 'import_data',
                    'table_name': table_name,
                    'source_file': source_file,
                    'imported_rows': len(df_cleaned),
                    'timestamp': datetime.now().isoformat()
                })

            conn.close()
            logger.info("âœ… æ•°æ®å¯¼å…¥å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
            raise

    def _map_fields_with_llm_guidance(self, df: pd.DataFrame, schema: Dict[str, Any]) -> pd.DataFrame:
        """æ ¹æ®LLMåˆ†æç»“æœè¿›è¡Œå­—æ®µæ˜ å°„"""

        field_mapping = {}
        fields = schema.get('fields', [])

        for field in fields:
            original_name = field.get('original_name')
            standard_name = field.get('standard_name')

            if original_name and standard_name and original_name in df.columns:
                field_mapping[original_name] = standard_name

        # é‡å‘½ååˆ—
        df_mapped = df.rename(columns=field_mapping)

        # åªä¿ç•™æ˜ å°„çš„åˆ—
        mapped_columns = list(field_mapping.values())
        existing_columns = [col for col in mapped_columns if col in df_mapped.columns]
        df_mapped = df_mapped[existing_columns]

        return df_mapped

    def _clean_data_with_llm_guidance(self, df: pd.DataFrame, schema: Dict[str, Any]) -> pd.DataFrame:
        """æ ¹æ®LLMåˆ†æç»“æœè¿›è¡Œæ•°æ®æ¸…æ´—"""

        fields = schema.get('fields', [])

        for field in fields:
            field_name = field.get('standard_name')
            data_type = field.get('data_type', '')
            is_required = field.get('is_required', False)

            if field_name not in df.columns:
                continue

            # æ•°æ®ç±»å‹è½¬æ¢
            if 'INTEGER' in data_type.upper():
                df[field_name] = pd.to_numeric(df[field_name], errors='coerce')
                df[field_name] = df[field_name].fillna(0).astype(int)

            elif 'DECIMAL' in data_type.upper() or 'REAL' in data_type.upper():
                df[field_name] = pd.to_numeric(df[field_name], errors='coerce')
                df[field_name] = df[field_name].fillna(0.0)

            elif 'VARCHAR' in data_type.upper() or 'TEXT' in data_type.upper():
                df[field_name] = df[field_name].astype(str)
                df[field_name] = df[field_name].replace('nan', '')

                # æˆªæ–­è¿‡é•¿çš„å­—ç¬¦ä¸²
                if 'VARCHAR' in data_type.upper():
                    length_match = re.search(r'VARCHAR\((\d+)\)', data_type.upper())
                    if length_match:
                        max_length = int(length_match.group(1))
                        df[field_name] = df[field_name].str[:max_length]

            # å¤„ç†å¿…å¡«å­—æ®µ
            if is_required:
                if df[field_name].dtype == 'object':
                    df[field_name] = df[field_name].fillna('')
                else:
                    df[field_name] = df[field_name].fillna(0)

        return df

    def _generate_context_configuration(self, output_db_path: str):
        """
        ç¬¬6æ­¥: ç”Ÿæˆä¸Šä¸‹æ–‡é…ç½®
        """
        logger.info("âš™ï¸ å¼€å§‹ç”Ÿæˆä¸Šä¸‹æ–‡é…ç½®")

        try:
            # ç”Ÿæˆæ•°æ®åº“ä¸Šä¸‹æ–‡é…ç½®
            context_config = self._build_context_config(output_db_path)

            # ä¿å­˜é…ç½®æ–‡ä»¶
            config_file_path = self._save_context_config(context_config, output_db_path)

            logger.info(f"âœ… ä¸Šä¸‹æ–‡é…ç½®å·²ä¿å­˜: {config_file_path}")

        except Exception as e:
            logger.error(f"âŒ ä¸Šä¸‹æ–‡é…ç½®ç”Ÿæˆå¤±è´¥: {e}")

    def _build_context_config(self, output_db_path: str) -> Dict[str, Any]:
        """æ„å»ºä¸Šä¸‹æ–‡é…ç½®"""

        # æ”¶é›†æ‰€æœ‰ä¸šåŠ¡æœ¯è¯­
        all_business_terms = {}
        all_relationships = {}
        all_field_mappings = {}

        for table_name, schema in self.database_schema.items():
            # ä¸šåŠ¡æœ¯è¯­
            for term in schema.get('business_terms', []):
                term_name = term.get('term_name')
                if term_name:
                    all_business_terms[term_name] = {
                        'name': term_name,
                        'definition': term.get('definition', ''),
                        'sql_conditions': term.get('sql_condition', ''),
                        'applicable_tables': [table_name],
                        'applicable_fields': term.get('applicable_fields', [])
                    }

            # è¡¨å…³ç³»
            for rel in schema.get('relationships', []):
                rel_name = f"{table_name}_{rel.get('type', 'relation')}"
                all_relationships[rel_name] = rel

            # å­—æ®µæ˜ å°„
            for field in schema.get('fields', []):
                field_name = field.get('standard_name')
                if field_name:
                    all_field_mappings[field_name] = {
                        'field_name': field_name,
                        'chinese_name': field.get('chinese_name', ''),
                        'business_meaning': field.get('business_meaning', ''),
                        'data_type': field.get('data_type', ''),
                        'table_name': table_name
                    }

        # æ„å»ºå®Œæ•´é…ç½®
        context_config = {
            'database_path': output_db_path,
            'database_name': os.path.splitext(os.path.basename(output_db_path))[0],
            'database_type': 'banking',
            'description': 'LLMæ™ºèƒ½åˆ†æç”Ÿæˆçš„é“¶è¡Œä¸šåŠ¡æ•°æ®åº“',
            'generated_at': datetime.now().isoformat(),
            'generation_method': 'llm_intelligent_analysis',

            # è¡¨ç»“æ„ä¿¡æ¯
            'tables': self._generate_table_info(),

            # ä¸šåŠ¡æœ¯è¯­
            'database_specific_business_terms': all_business_terms,

            # å­—æ®µæ˜ å°„
            'database_specific_field_mappings': all_field_mappings,

            # è¡¨å…³ç³»
            'relationships': all_relationships,

            # LLMåˆ†æç»“æœ
            'llm_analysis_summary': self._generate_analysis_summary()
        }

        return context_config

    def _generate_table_info(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¡¨ä¿¡æ¯"""
        tables_info = {}

        for table_name, schema in self.database_schema.items():
            fields = schema.get('fields', [])

            tables_info[table_name] = {
                'name': table_name,
                'description': schema.get('description', ''),
                'columns': [
                    {
                        'name': field.get('standard_name', ''),
                        'type': field.get('data_type', ''),
                        'nullable': not field.get('is_required', False),
                        'is_primary_key': field.get('is_primary_key', False),
                        'chinese_name': field.get('chinese_name', ''),
                        'business_meaning': field.get('business_meaning', '')
                    }
                    for field in fields
                ],
                'primary_keys': [
                    field.get('standard_name', '')
                    for field in fields
                    if field.get('is_primary_key', False)
                ],
                'source_file': schema.get('source_file', '')
            }

        return tables_info

    def _generate_analysis_summary(self) -> Dict[str, Any]:
        """ç”ŸæˆLLMåˆ†ææ‘˜è¦"""
        summary = {
            'total_files_analyzed': len(self.llm_analysis_results),
            'total_tables_generated': len(self.database_schema),
            'analysis_timestamp': datetime.now().isoformat(),
            'llm_model': 'deepseek-chat',
            'analysis_details': {}
        }

        for file_name, analysis in self.llm_analysis_results.items():
            summary['analysis_details'][file_name] = {
                'table_name': analysis.get('table_name', ''),
                'fields_count': len(analysis.get('fields', [])),
                'business_terms_count': len(analysis.get('business_terms', [])),
                'relationships_count': len(analysis.get('relationships', [])),
                'data_quality_issues': analysis.get('data_quality_issues', []),
                'recommendations': analysis.get('recommendations', [])
            }

        return summary

    def _save_context_config(self, context_config: Dict[str, Any], output_db_path: str) -> str:
        """ä¿å­˜ä¸Šä¸‹æ–‡é…ç½®æ–‡ä»¶"""

        # ç”Ÿæˆé…ç½®æ–‡ä»¶è·¯å¾„
        db_name = os.path.splitext(os.path.basename(output_db_path))[0]
        db_hash = hashlib.md5(output_db_path.encode()).hexdigest()

        config_dir = "configs/database_contexts"
        os.makedirs(config_dir, exist_ok=True)

        config_file_path = os.path.join(config_dir, f"{db_name}_{db_hash}.json")

        # ä¿å­˜é…ç½®æ–‡ä»¶
        with open(config_file_path, 'w', encoding='utf-8') as f:
            json.dump(context_config, f, ensure_ascii=False, indent=2)

        return config_file_path

    def _generate_comprehensive_report(self, output_db_path: str, start_time: float) -> Dict[str, Any]:
        """
        ç¬¬7æ­¥: ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        """
        logger.info("ğŸ“‹ ç”Ÿæˆå¤„ç†æŠ¥å‘Š")

        execution_time = time.time() - start_time

        # ç»Ÿè®¡å¯¼å…¥æ•°æ®
        total_imported_rows = sum(
            log['imported_rows'] for log in self.import_log
            if log['step'] == 'import_data'
        )

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = {
            'success': True,
            'execution_time': execution_time,
            'output_database': output_db_path,
            'database_name': os.path.splitext(os.path.basename(output_db_path))[0],

            # å¤„ç†ç»Ÿè®¡
            'processing_summary': {
                'total_source_files': len(self.source_files),
                'total_dictionary_files': len(self.dictionary_files),
                'total_tables_created': len(self.database_schema),
                'total_imported_rows': total_imported_rows,
                'llm_analysis_success_rate': len(self.llm_analysis_results) / len(self.source_files) if self.source_files else 0
            },

            # æ–‡ä»¶å¤„ç†è¯¦æƒ…
            'file_processing_details': {
                'source_files': list(self.source_files.keys()),
                'dictionary_files': list(self.dictionary_files.keys()),
                'successfully_analyzed': list(self.llm_analysis_results.keys())
            },

            # æ•°æ®åº“ç»“æ„
            'database_structure': {
                table_name: {
                    'description': schema.get('description', ''),
                    'fields_count': len(schema.get('fields', [])),
                    'source_file': schema.get('source_file', '')
                }
                for table_name, schema in self.database_schema.items()
            },

            # LLMåˆ†ææ‘˜è¦
            'llm_analysis_summary': self._generate_analysis_summary(),

            # è¯¦ç»†æ—¥å¿—
            'detailed_log': self.import_log,

            # ç”Ÿæˆçš„ä¸šåŠ¡æœ¯è¯­
            'generated_business_terms': self._extract_all_business_terms(),

            # å»ºè®®å’Œé—®é¢˜
            'recommendations': self._generate_final_recommendations(),
            'data_quality_issues': self._collect_all_data_quality_issues()
        }

        logger.info("âœ… å¤„ç†æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return report

    def _extract_all_business_terms(self) -> List[Dict[str, Any]]:
        """æå–æ‰€æœ‰ç”Ÿæˆçš„ä¸šåŠ¡æœ¯è¯­"""
        all_terms = []

        for analysis in self.llm_analysis_results.values():
            terms = analysis.get('business_terms', [])
            all_terms.extend(terms)

        return all_terms

    def _generate_final_recommendations(self) -> List[str]:
        """ç”Ÿæˆæœ€ç»ˆå»ºè®®"""
        recommendations = []

        # æ”¶é›†æ‰€æœ‰LLMå»ºè®®
        for analysis in self.llm_analysis_results.values():
            recs = analysis.get('recommendations', [])
            recommendations.extend(recs)

        # æ·»åŠ é€šç”¨å»ºè®®
        recommendations.extend([
            "å»ºè®®å®šæœŸéªŒè¯ç”Ÿæˆçš„ä¸šåŠ¡æœ¯è¯­å®šä¹‰çš„å‡†ç¡®æ€§",
            "å»ºè®®æ ¹æ®å®é™…ä¸šåŠ¡éœ€æ±‚è°ƒæ•´å­—æ®µæ˜ å°„å’Œæ•°æ®ç±»å‹",
            "å»ºè®®å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§æœºåˆ¶",
            "å»ºè®®å®šæœŸæ›´æ–°æ•°æ®å­—å…¸ä»¥ä¿æŒåŒæ­¥"
        ])

        return list(set(recommendations))  # å»é‡

    def _collect_all_data_quality_issues(self) -> List[str]:
        """æ”¶é›†æ‰€æœ‰æ•°æ®è´¨é‡é—®é¢˜"""
        all_issues = []

        for analysis in self.llm_analysis_results.values():
            issues = analysis.get('data_quality_issues', [])
            all_issues.extend(issues)

        return list(set(all_issues))  # å»é‡


# ä¾¿æ·å‡½æ•°
def create_llm_intelligent_importer(api_key: Optional[str] = None) -> LLMIntelligentDataImporter:
    """åˆ›å»ºLLMæ™ºèƒ½æ•°æ®å¯¼å…¥å™¨å®ä¾‹"""
    return LLMIntelligentDataImporter(api_key)


def quick_llm_import(data_source_dir: str, data_dict_dir: str, output_db_path: str, api_key: Optional[str] = None) -> Dict[str, Any]:
    """å¿«é€ŸLLMæ™ºèƒ½å¯¼å…¥å‡½æ•°"""
    importer = create_llm_intelligent_importer(api_key)
    return importer.process_batch_import(data_source_dir, data_dict_dir, output_db_path)
